# RAG æ£€ç´¢å¢å¼ºç³»ç»Ÿå®ç°

> **æ–‡æ¡£ç‰ˆæœ¬**: v1.0
> **åˆ›å»ºæ—¶é—´**: 2025-10-27
> **å®æ–½çŠ¶æ€**: è®¾è®¡é˜¶æ®µ
> **è´Ÿè´£äºº**: AIæ¶æ„ç»„

---

## ğŸ“‹ æ–‡æ¡£æ¦‚è¿°

æœ¬æ–‡æ¡£è¯¦ç»†è®¾è®¡é’ç¾½å¹³å°çš„ RAGï¼ˆRetrieval Augmented Generationï¼‰æ£€ç´¢å¢å¼ºç³»ç»Ÿï¼Œç”¨äºæ™ºèƒ½å†™ä½œç»­å†™ã€è®¾å®šé—®ç­”ã€é˜…è¯»åŠ©æ‰‹ç­‰åœºæ™¯ã€‚åŸºäºå‘é‡æ£€ç´¢å’Œæ··åˆæœç´¢æŠ€æœ¯ï¼Œä¸º AI Agent æä¾›ç²¾å‡†çš„ä¸Šä¸‹æ–‡å¢å¼ºèƒ½åŠ›ã€‚

**é€‚ç”¨èŒƒå›´**ï¼š
- å‘é‡åŒ–å¼•æ“å®ç°
- çŸ¥è¯†åº“ç®¡ç†
- æ··åˆæ£€ç´¢ç­–ç•¥
- ä¸ Agent çš„é›†æˆ

---

## ğŸ¯ è®¾è®¡ç›®æ ‡

### æ ¸å¿ƒç›®æ ‡

1. **é«˜ç²¾åº¦æ£€ç´¢**ï¼šè¯­ä¹‰ç›¸ä¼¼åº¦ + å…³é”®è¯åŒ¹é…ï¼Œæ£€ç´¢å‡†ç¡®ç‡ â‰¥85%
2. **ä½å»¶è¿Ÿå“åº”**ï¼šP95 å»¶è¿Ÿ < 200ms
3. **å¤§è§„æ¨¡æ”¯æŒ**ï¼šæ”¯æŒç™¾ä¸‡çº§æ–‡æ¡£ç´¢å¼•
4. **çµæ´»æ‰©å±•**ï¼šæ˜“äºæ·»åŠ æ–°çš„æ£€ç´¢ç­–ç•¥å’Œé‡æ’åºç®—æ³•
5. **äº‹ä»¶é©±åŠ¨ç´¢å¼•**ï¼šå®æ—¶æ›´æ–°ç´¢å¼•ï¼Œä¿æŒæ•°æ®ä¸€è‡´æ€§

### éç›®æ ‡

- âŒ ä¸å®ç°åˆ†å¸ƒå¼å‘é‡æ•°æ®åº“ï¼ˆä½¿ç”¨ Milvusï¼‰
- âŒ ä¸è®­ç»ƒè‡ªå®šä¹‰ Embedding æ¨¡å‹ï¼ˆä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ï¼‰

---

## ä¸€ã€ç³»ç»Ÿæ¶æ„

### 1.1 æ•´ä½“æ¶æ„å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   RAG System Architecture                     â”‚
â”‚                                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚         Vectorization Engine (å‘é‡åŒ–å¼•æ“)            â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚  â”‚  Text Preprocessing                            â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  - HTML/Markdown æ¸…ç†                          â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  - ç©ºç™½å­—ç¬¦æ ‡å‡†åŒ–                              â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  - ç‰¹æ®Šå­—ç¬¦å¤„ç†                                â”‚  â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚  â”‚               â”‚                                       â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚  â”‚  Chunk Splitter (æ™ºèƒ½åˆ†å—)                    â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  - æŒ‰æ®µè½åˆ†å‰²                                  â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  - è¯­ä¹‰è¾¹ç•Œæ£€æµ‹                                â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  - é‡å åˆ†å—ç­–ç•¥                                â”‚  â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚  â”‚               â”‚                                       â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚  â”‚  Embedding Model (BGE-large-zh-v1.5)          â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  - Batch Processing                            â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  - 1024 ç»´å‘é‡                                 â”‚  â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                     â”‚                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚        Knowledge Base Manager (çŸ¥è¯†åº“ç®¡ç†)          â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚  â”‚  Document Model                                â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  - document_id, user_id, project_id            â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  - content, content_type, metadata             â”‚  â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚  â”‚  VectorDocument Model                          â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  - chunk_id, document_id, chunk_index          â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  - chunk_text, embedding, metadata             â”‚  â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                     â”‚                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚          Milvus Vector Database                       â”‚   â”‚
â”‚  â”‚  Collection: user_private_kb                          â”‚   â”‚
â”‚  â”‚  Collection: platform_kb                              â”‚   â”‚
â”‚  â”‚  Index: HNSW (M=16, efConstruction=128)              â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                     â”‚                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚        Retrieval Engine (æ£€ç´¢å¼•æ“)                   â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚  â”‚  Semantic Search (å‘é‡ç›¸ä¼¼åº¦æ£€ç´¢)             â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  - HNSW ç´¢å¼•å¿«é€Ÿæœç´¢                          â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  - ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—                             â”‚  â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚  â”‚  Keyword Search (BM25 å…³é”®è¯æ£€ç´¢)            â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  - åŸºäº jieba åˆ†è¯                            â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  - TF-IDF æƒé‡                                â”‚  â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚  â”‚  Hybrid Search (æ··åˆæ£€ç´¢)                     â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  - RRF (Reciprocal Rank Fusion)               â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  - åŠ æƒèåˆç­–ç•¥                               â”‚  â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚  â”‚  Reranker (é‡æ’åº)                            â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  - BGE-reranker-large                         â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  - Cross-encoder ç²¾å‡†æ‰“åˆ†                      â”‚  â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 æ ¸å¿ƒç»„ä»¶

| ç»„ä»¶ | èŒè´£ | æŠ€æœ¯é€‰å‹ |
|------|------|---------|
| **Vectorization Engine** | æ–‡æœ¬é¢„å¤„ç†å’Œå‘é‡åŒ– | sentence-transformers |
| **Embedding Model** | æ–‡æœ¬å‘é‡ç¼–ç  | BGE-large-zh-v1.5 |
| **Vector Database** | å‘é‡å­˜å‚¨å’Œæ£€ç´¢ | Milvus |
| **Keyword Search** | å…³é”®è¯åŒ¹é… | jieba + BM25 |
| **Reranker** | ç»“æœé‡æ’åº | BGE-reranker-large |

---

## äºŒã€å‘é‡åŒ–å¼•æ“å®ç°

### 2.1 æ–‡æœ¬é¢„å¤„ç†

```python
# src/core/rag/preprocessing.py
import re
from typing import List
from bs4 import BeautifulSoup
import markdown


class TextPreprocessor:
    """æ–‡æœ¬é¢„å¤„ç†å™¨"""
    
    def preprocess(self, text: str, content_type: str = "text") -> str:
        """é¢„å¤„ç†æ–‡æœ¬
        
        Args:
            text: åŸå§‹æ–‡æœ¬
            content_type: å†…å®¹ç±»å‹ (text, html, markdown)
            
        Returns:
            å¤„ç†åçš„æ–‡æœ¬
        """
        # 1. æ ¹æ®ç±»å‹æ¸…ç†
        if content_type == "html":
            text = self._clean_html(text)
        elif content_type == "markdown":
            text = self._clean_markdown(text)
        
        # 2. æ ‡å‡†åŒ–ç©ºç™½å­—ç¬¦
        text = self._normalize_whitespace(text)
        
        # 3. ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼ˆä¿ç•™ä¸­è‹±æ–‡å’Œæ ‡ç‚¹ï¼‰
        text = self._clean_special_chars(text)
        
        return text.strip()
    
    def _clean_html(self, html: str) -> str:
        """æ¸…ç† HTML æ ‡ç­¾"""
        soup = BeautifulSoup(html, 'html.parser')
        # ç§»é™¤ script å’Œ style æ ‡ç­¾
        for script in soup(["script", "style"]):
            script.decompose()
        return soup.get_text()
    
    def _clean_markdown(self, md: str) -> str:
        """æ¸…ç† Markdown æ ¼å¼"""
        # è½¬æ¢ä¸º HTML å†æå–æ–‡æœ¬
        html = markdown.markdown(md)
        return self._clean_html(html)
    
    def _normalize_whitespace(self, text: str) -> str:
        """æ ‡å‡†åŒ–ç©ºç™½å­—ç¬¦"""
        # ç»Ÿä¸€æ¢è¡Œç¬¦
        text = text.replace('\r\n', '\n').replace('\r', '\n')
        # ç§»é™¤å¤šä½™ç©ºç™½
        text = re.sub(r'[ \t]+', ' ', text)
        # ç§»é™¤å¤šä½™æ¢è¡Œ
        text = re.sub(r'\n{3,}', '\n\n', text)
        return text
    
    def _clean_special_chars(self, text: str) -> str:
        """æ¸…ç†ç‰¹æ®Šå­—ç¬¦"""
        # ä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—ã€åŸºæœ¬æ ‡ç‚¹
        text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9\s\.,;:!?()ï¼ˆï¼‰ã€ã€‘ã€Œã€""''â€¦â€”\-]', '', text)
        return text
```

### 2.2 æ™ºèƒ½åˆ†å—ç­–ç•¥

```python
# src/core/rag/chunking.py
from typing import List, Tuple
import jieba


class ChunkSplitter:
    """æ™ºèƒ½åˆ†å—å™¨"""
    
    def __init__(
        self,
        chunk_size: int = 500,
        chunk_overlap: int = 50,
        separators: List[str] = None
    ):
        """åˆå§‹åŒ–
        
        Args:
            chunk_size: å—å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰
            chunk_overlap: é‡å å¤§å°
            separators: åˆ†éš”ç¬¦åˆ—è¡¨
        """
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.separators = separators or ['\n\n', '\n', 'ã€‚', 'ï¼', 'ï¼Ÿ', ';', 'ï¼›']
    
    def split(self, text: str) -> List[Tuple[str, int]]:
        """åˆ†å—
        
        Args:
            text: æ–‡æœ¬
            
        Returns:
            [(chunk_text, start_offset), ...]
        """
        chunks = []
        
        # 1. æŒ‰æ®µè½åˆ†å‰²
        paragraphs = self._split_by_separators(text)
        
        # 2. ç»„è£… chunks
        current_chunk = ""
        current_offset = 0
        
        for para, offset in paragraphs:
            # å¦‚æœå½“å‰å—åŠ ä¸Šæ–°æ®µè½ä¸è¶…è¿‡é™åˆ¶ï¼Œç›´æ¥æ·»åŠ 
            if len(current_chunk) + len(para) <= self.chunk_size:
                current_chunk += para + "\n\n"
            else:
                # ä¿å­˜å½“å‰å—
                if current_chunk:
                    chunks.append((current_chunk.strip(), current_offset))
                
                # å¦‚æœå•ä¸ªæ®µè½è¶…è¿‡ chunk_sizeï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ‡åˆ†
                if len(para) > self.chunk_size:
                    sub_chunks = self._split_long_paragraph(para)
                    chunks.extend([(c, offset) for c in sub_chunks])
                    current_chunk = ""
                    current_offset = offset + len(para)
                else:
                    current_chunk = para + "\n\n"
                    current_offset = offset
        
        # æ·»åŠ æœ€åä¸€ä¸ªå—
        if current_chunk:
            chunks.append((current_chunk.strip(), current_offset))
        
        # 3. æ·»åŠ é‡å 
        chunks = self._add_overlap(chunks)
        
        return chunks
    
    def _split_by_separators(self, text: str) -> List[Tuple[str, int]]:
        """æŒ‰åˆ†éš”ç¬¦åˆ†å‰²"""
        parts = []
        current = ""
        offset = 0
        
        for separator in self.separators:
            if separator in text:
                segments = text.split(separator)
                for i, seg in enumerate(segments):
                    if i < len(segments) - 1:
                        parts.append((seg + separator, offset))
                        offset += len(seg) + len(separator)
                    else:
                        parts.append((seg, offset))
                break
        
        if not parts:
            parts = [(text, 0)]
        
        return parts
    
    def _split_long_paragraph(self, para: str) -> List[str]:
        """åˆ†å‰²é•¿æ®µè½ï¼ˆæŒ‰å¥å­ï¼‰"""
        # ä½¿ç”¨ jieba åˆ†å¥
        sentences = []
        current = ""
        
        for char in para:
            current += char
            if char in ['ã€‚', 'ï¼', 'ï¼Ÿ', '\n']:
                sentences.append(current)
                current = ""
        
        if current:
            sentences.append(current)
        
        # ç»„è£…æˆ chunk_size å¤§å°çš„å—
        chunks = []
        current_chunk = ""
        
        for sentence in sentences:
            if len(current_chunk) + len(sentence) <= self.chunk_size:
                current_chunk += sentence
            else:
                if current_chunk:
                    chunks.append(current_chunk)
                current_chunk = sentence
        
        if current_chunk:
            chunks.append(current_chunk)
        
        return chunks
    
    def _add_overlap(self, chunks: List[Tuple[str, int]]) -> List[Tuple[str, int]]:
        """æ·»åŠ é‡å """
        if len(chunks) <= 1:
            return chunks
        
        overlapped = []
        for i, (chunk, offset) in enumerate(chunks):
            if i > 0:
                # æ·»åŠ å‰ä¸€ä¸ªå—çš„å°¾éƒ¨
                prev_chunk = chunks[i-1][0]
                overlap_text = prev_chunk[-self.chunk_overlap:]
                chunk = overlap_text + chunk
            
            overlapped.append((chunk, offset))
        
        return overlapped
```

### 2.3 å‘é‡ç¼–ç 

```python
# src/core/rag/vectorizer.py
from typing import List
import numpy as np
from sentence_transformers import SentenceTransformer
from utils.logging import get_logger

logger = get_logger(__name__)


class EmbeddingEngine:
    """å‘é‡ç¼–ç å¼•æ“"""
    
    def __init__(
        self,
        model_name: str = "BAAI/bge-large-zh-v1.5",
        batch_size: int = 32,
        device: str = "cuda"
    ):
        """åˆå§‹åŒ–
        
        Args:
            model_name: æ¨¡å‹åç§°
            batch_size: æ‰¹å¤„ç†å¤§å°
            device: è®¾å¤‡ (cuda/cpu)
        """
        self.model_name = model_name
        self.batch_size = batch_size
        self.device = device
        
        logger.info(f"Loading embedding model: {model_name}")
        self.model = SentenceTransformer(model_name, device=device)
        logger.info("Embedding model loaded")
    
    def encode(
        self,
        texts: List[str],
        show_progress: bool = False
    ) -> np.ndarray:
        """æ‰¹é‡ç¼–ç 
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦
            
        Returns:
            å‘é‡æ•°ç»„ (n, dim)
        """
        logger.info(f"Encoding {len(texts)} texts...")
        
        embeddings = self.model.encode(
            texts,
            batch_size=self.batch_size,
            show_progress_bar=show_progress,
            normalize_embeddings=True  # å½’ä¸€åŒ–ï¼Œä¾¿äºä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—
        )
        
        logger.info(f"Encoding completed, shape: {embeddings.shape}")
        return embeddings
    
    def encode_query(self, query: str) -> np.ndarray:
        """ç¼–ç æŸ¥è¯¢ï¼ˆæ·»åŠ æŒ‡ä»¤å‰ç¼€ï¼‰"""
        # BGE æ¨¡å‹æ¨èä¸ºæŸ¥è¯¢æ·»åŠ æŒ‡ä»¤
        query_with_instruction = f"ä¸ºè¿™ä¸ªå¥å­ç”Ÿæˆè¡¨ç¤ºä»¥ç”¨äºæ£€ç´¢ç›¸å…³æ–‡ç« ï¼š{query}"
        return self.encode([query_with_instruction])[0]
```

---

## ä¸‰ã€çŸ¥è¯†åº“ç®¡ç†

### 3.1 æ•°æ®æ¨¡å‹

```python
# src/models/domain.py
from dataclasses import dataclass, field
from datetime import datetime
from typing import List, Dict, Any, Optional


@dataclass
class Document:
    """æ–‡æ¡£æ¨¡å‹"""
    id: str
    user_id: str
    project_id: str
    content: str
    content_type: str  # 'chapter', 'character', 'setting', 'outline', 'timeline'
    metadata: Dict[str, Any] = field(default_factory=dict)
    created_at: datetime = field(default_factory=datetime.now)
    updated_at: datetime = field(default_factory=datetime.now)


@dataclass
class VectorDocument:
    """å‘é‡æ–‡æ¡£æ¨¡å‹"""
    id: str  # chunk_id
    document_id: str
    chunk_index: int
    chunk_text: str
    embedding: List[float]
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    def to_milvus_entity(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸º Milvus å®ä½“"""
        return {
            "id": self.id,
            "document_id": self.document_id,
            "chunk_index": self.chunk_index,
            "chunk_text": self.chunk_text,
            "embedding": self.embedding,
            "user_id": self.metadata.get("user_id", ""),
            "project_id": self.metadata.get("project_id", ""),
            "content_type": self.metadata.get("content_type", ""),
            "metadata_json": json.dumps(self.metadata)
        }
```

### 3.2 Milvus å®¢æˆ·ç«¯

```python
# src/infrastructure/vector_db/milvus_client.py
from pymilvus import (
    connections,
    Collection,
    CollectionSchema,
    FieldSchema,
    DataType,
    utility
)
from typing import List, Dict, Any, Tuple
from utils.logging import get_logger

logger = get_logger(__name__)


class MilvusClient:
    """Milvus å®¢æˆ·ç«¯"""
    
    COLLECTION_NAME = "qingyu_kb"
    VECTOR_DIM = 1024  # BGE-large-zh-v1.5 çš„ç»´åº¦
    
    def __init__(
        self,
        host: str = "localhost",
        port: int = 19530,
        user: str = "",
        password: str = ""
    ):
        """åˆå§‹åŒ–"""
        self.host = host
        self.port = port
        
        # è¿æ¥
        logger.info(f"Connecting to Milvus at {host}:{port}")
        connections.connect(
            alias="default",
            host=host,
            port=port,
            user=user,
            password=password
        )
        
        # åˆ›å»º collection
        self._create_collection_if_not_exists()
        
        # åŠ è½½ collection
        self.collection = Collection(self.COLLECTION_NAME)
        self.collection.load()
        
        logger.info(f"Milvus client initialized, collection: {self.COLLECTION_NAME}")
    
    def _create_collection_if_not_exists(self):
        """åˆ›å»º collectionï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰"""
        if utility.has_collection(self.COLLECTION_NAME):
            logger.info(f"Collection {self.COLLECTION_NAME} already exists")
            return
        
        # å®šä¹‰ schema
        fields = [
            FieldSchema(name="id", dtype=DataType.VARCHAR, is_primary=True, max_length=100),
            FieldSchema(name="document_id", dtype=DataType.VARCHAR, max_length=100),
            FieldSchema(name="chunk_index", dtype=DataType.INT64),
            FieldSchema(name="chunk_text", dtype=DataType.VARCHAR, max_length=10000),
            FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=self.VECTOR_DIM),
            FieldSchema(name="user_id", dtype=DataType.VARCHAR, max_length=100),
            FieldSchema(name="project_id", dtype=DataType.VARCHAR, max_length=100),
            FieldSchema(name="content_type", dtype=DataType.VARCHAR, max_length=50),
            FieldSchema(name="metadata_json", dtype=DataType.VARCHAR, max_length=5000),
        ]
        
        schema = CollectionSchema(
            fields=fields,
            description="é’ç¾½çŸ¥è¯†åº“"
        )
        
        # åˆ›å»º collection
        collection = Collection(
            name=self.COLLECTION_NAME,
            schema=schema
        )
        
        # åˆ›å»ºç´¢å¼•
        index_params = {
            "metric_type": "COSINE",  # ä½™å¼¦ç›¸ä¼¼åº¦
            "index_type": "HNSW",
            "params": {
                "M": 16,
                "efConstruction": 128
            }
        }
        
        collection.create_index(
            field_name="embedding",
            index_params=index_params
        )
        
        logger.info(f"Collection {self.COLLECTION_NAME} created with HNSW index")
    
    def insert(self, vector_docs: List[VectorDocument]) -> List[str]:
        """æ’å…¥å‘é‡æ–‡æ¡£
        
        Args:
            vector_docs: å‘é‡æ–‡æ¡£åˆ—è¡¨
            
        Returns:
            æ’å…¥çš„ ID åˆ—è¡¨
        """
        if not vector_docs:
            return []
        
        entities = [doc.to_milvus_entity() for doc in vector_docs]
        
        # è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼
        data = [
            [e["id"] for e in entities],
            [e["document_id"] for e in entities],
            [e["chunk_index"] for e in entities],
            [e["chunk_text"] for e in entities],
            [e["embedding"] for e in entities],
            [e["user_id"] for e in entities],
            [e["project_id"] for e in entities],
            [e["content_type"] for e in entities],
            [e["metadata_json"] for e in entities],
        ]
        
        result = self.collection.insert(data)
        self.collection.flush()
        
        logger.info(f"Inserted {len(vector_docs)} vector documents")
        return result.primary_keys
    
    def search(
        self,
        query_vector: List[float],
        top_k: int = 10,
        filters: Dict[str, Any] = None
    ) -> List[Tuple[str, float, Dict[str, Any]]]:
        """æœç´¢
        
        Args:
            query_vector: æŸ¥è¯¢å‘é‡
            top_k: è¿”å›æ•°é‡
            filters: è¿‡æ»¤æ¡ä»¶ {"user_id": "xxx", "project_id": "yyy"}
            
        Returns:
            [(chunk_id, score, metadata), ...]
        """
        # æ„å»ºè¿‡æ»¤è¡¨è¾¾å¼
        expr = self._build_filter_expr(filters)
        
        # æœç´¢
        search_params = {
            "metric_type": "COSINE",
            "params": {"ef": 64}
        }
        
        results = self.collection.search(
            data=[query_vector],
            anns_field="embedding",
            param=search_params,
            limit=top_k,
            expr=expr,
            output_fields=["document_id", "chunk_text", "user_id", "project_id", "content_type", "metadata_json"]
        )
        
        # è½¬æ¢ç»“æœ
        hits = []
        for hit in results[0]:
            hits.append((
                hit.id,
                hit.score,
                {
                    "document_id": hit.entity.get("document_id"),
                    "chunk_text": hit.entity.get("chunk_text"),
                    "user_id": hit.entity.get("user_id"),
                    "project_id": hit.entity.get("project_id"),
                    "content_type": hit.entity.get("content_type"),
                    "metadata": json.loads(hit.entity.get("metadata_json", "{}"))
                }
            ))
        
        logger.info(f"Search returned {len(hits)} results")
        return hits
    
    def delete_by_document_id(self, document_id: str):
        """åˆ é™¤æ–‡æ¡£çš„æ‰€æœ‰å‘é‡"""
        expr = f'document_id == "{document_id}"'
        self.collection.delete(expr)
        self.collection.flush()
        logger.info(f"Deleted vectors for document: {document_id}")
    
    def _build_filter_expr(self, filters: Dict[str, Any]) -> str:
        """æ„å»ºè¿‡æ»¤è¡¨è¾¾å¼"""
        if not filters:
            return ""
        
        conditions = []
        for key, value in filters.items():
            if isinstance(value, str):
                conditions.append(f'{key} == "{value}"')
            elif isinstance(value, (int, float)):
                conditions.append(f'{key} == {value}')
            elif isinstance(value, list):
                # IN æŸ¥è¯¢
                values_str = ", ".join([f'"{v}"' if isinstance(v, str) else str(v) for v in value])
                conditions.append(f'{key} in [{values_str}]')
        
        return " && ".join(conditions)
```

### 3.3 çŸ¥è¯†åº“ç®¡ç†å™¨

```python
# src/core/rag/knowledge_base.py
from typing import List, Dict, Any
from models.domain import Document, VectorDocument
from core.rag.preprocessing import TextPreprocessor
from core.rag.chunking import ChunkSplitter
from core.rag.vectorizer import EmbeddingEngine
from infrastructure.vector_db.milvus_client import MilvusClient
from utils.logging import get_logger

logger = get_logger(__name__)


class KnowledgeBaseManager:
    """çŸ¥è¯†åº“ç®¡ç†å™¨"""
    
    def __init__(
        self,
        vectorizer: EmbeddingEngine,
        vector_db: MilvusClient,
        preprocessor: TextPreprocessor = None,
        chunker: ChunkSplitter = None
    ):
        """åˆå§‹åŒ–"""
        self.vectorizer = vectorizer
        self.vector_db = vector_db
        self.preprocessor = preprocessor or TextPreprocessor()
        self.chunker = chunker or ChunkSplitter(chunk_size=500, chunk_overlap=50)
    
    async def index_document(self, document: Document) -> int:
        """ç´¢å¼•æ–‡æ¡£
        
        Args:
            document: æ–‡æ¡£
            
        Returns:
            ç´¢å¼•çš„å—æ•°é‡
        """
        logger.info(f"Indexing document: {document.id}")
        
        # 1. é¢„å¤„ç†
        cleaned_text = self.preprocessor.preprocess(
            document.content,
            content_type=document.metadata.get("format", "text")
        )
        
        # 2. åˆ†å—
        chunks = self.chunker.split(cleaned_text)
        logger.info(f"Document split into {len(chunks)} chunks")
        
        # 3. å‘é‡åŒ–
        chunk_texts = [chunk[0] for chunk in chunks]
        embeddings = self.vectorizer.encode(chunk_texts)
        
        # 4. æ„å»ºå‘é‡æ–‡æ¡£
        vector_docs = []
        for i, ((chunk_text, offset), embedding) in enumerate(zip(chunks, embeddings)):
            vector_doc = VectorDocument(
                id=f"{document.id}_{i}",
                document_id=document.id,
                chunk_index=i,
                chunk_text=chunk_text,
                embedding=embedding.tolist(),
                metadata={
                    "user_id": document.user_id,
                    "project_id": document.project_id,
                    "content_type": document.content_type,
                    "offset": offset,
                    **document.metadata
                }
            )
            vector_docs.append(vector_doc)
        
        # 5. æ’å…¥å‘é‡æ•°æ®åº“
        self.vector_db.insert(vector_docs)
        
        logger.info(f"Document indexed: {document.id}, {len(vector_docs)} chunks")
        return len(vector_docs)
    
    async def delete_document(self, document_id: str):
        """åˆ é™¤æ–‡æ¡£ç´¢å¼•"""
        logger.info(f"Deleting document index: {document_id}")
        self.vector_db.delete_by_document_id(document_id)
        logger.info(f"Document index deleted: {document_id}")
    
    async def batch_index(self, documents: List[Document]) -> Dict[str, int]:
        """æ‰¹é‡ç´¢å¼•"""
        results = {}
        for doc in documents:
            try:
                count = await self.index_document(doc)
                results[doc.id] = count
            except Exception as e:
                logger.error(f"Failed to index document {doc.id}: {e}")
                results[doc.id] = -1
        
        return results
```

---

## å››ã€æ£€ç´¢å¼•æ“å®ç°

### 4.1 æ··åˆæ£€ç´¢

```python
# src/core/rag/retriever.py
from typing import List, Dict, Any, Tuple
from dataclasses import dataclass
import jieba
import math
from collections import Counter
from utils.logging import get_logger

logger = get_logger(__name__)


@dataclass
class SearchResult:
    """æœç´¢ç»“æœ"""
    chunk_id: str
    document_id: str
    chunk_text: str
    score: float
    metadata: Dict[str, Any]
    highlights: List[str] = None


class HybridSearchEngine:
    """æ··åˆæ£€ç´¢å¼•æ“"""
    
    def __init__(
        self,
        vectorizer: EmbeddingEngine,
        vector_db: MilvusClient,
        semantic_weight: float = 0.7,
        keyword_weight: float = 0.3
    ):
        """åˆå§‹åŒ–
        
        Args:
            vectorizer: å‘é‡åŒ–å¼•æ“
            vector_db: å‘é‡æ•°æ®åº“
            semantic_weight: è¯­ä¹‰æ£€ç´¢æƒé‡
            keyword_weight: å…³é”®è¯æ£€ç´¢æƒé‡
        """
        self.vectorizer = vectorizer
        self.vector_db = vector_db
        self.semantic_weight = semantic_weight
        self.keyword_weight = keyword_weight
    
    async def search(
        self,
        query: str,
        user_id: str,
        project_id: str,
        top_k: int = 10,
        content_types: List[str] = None,
        enable_hybrid: bool = True
    ) -> List[SearchResult]:
        """æ··åˆæ£€ç´¢
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            user_id: ç”¨æˆ· ID
            project_id: é¡¹ç›® ID
            top_k: è¿”å›æ•°é‡
            content_types: å†…å®¹ç±»å‹è¿‡æ»¤
            enable_hybrid: æ˜¯å¦å¯ç”¨æ··åˆæ£€ç´¢
            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        logger.info(f"Hybrid search: query='{query[:50]}', top_k={top_k}")
        
        if not enable_hybrid:
            # åªä½¿ç”¨è¯­ä¹‰æ£€ç´¢
            return await self._semantic_search(query, user_id, project_id, top_k, content_types)
        
        # 1. è¯­ä¹‰æ£€ç´¢
        semantic_results = await self._semantic_search(
            query, user_id, project_id, top_k * 2, content_types
        )
        
        # 2. å…³é”®è¯æ£€ç´¢ï¼ˆBM25ï¼‰
        keyword_results = await self._keyword_search(
            query, semantic_results, top_k * 2
        )
        
        # 3. èåˆç»“æœï¼ˆRRFï¼‰
        fused_results = self._fuse_results(
            semantic_results,
            keyword_results,
            top_k
        )
        
        logger.info(f"Hybrid search returned {len(fused_results)} results")
        return fused_results
    
    async def _semantic_search(
        self,
        query: str,
        user_id: str,
        project_id: str,
        top_k: int,
        content_types: List[str] = None
    ) -> List[SearchResult]:
        """è¯­ä¹‰æ£€ç´¢"""
        # å‘é‡åŒ–æŸ¥è¯¢
        query_vector = self.vectorizer.encode_query(query)
        
        # æ„å»ºè¿‡æ»¤æ¡ä»¶
        filters = {
            "user_id": user_id,
            "project_id": project_id
        }
        if content_types:
            filters["content_type"] = content_types
        
        # æ£€ç´¢
        hits = self.vector_db.search(
            query_vector=query_vector.tolist(),
            top_k=top_k,
            filters=filters
        )
        
        # è½¬æ¢ä¸º SearchResult
        results = []
        for chunk_id, score, metadata in hits:
            results.append(SearchResult(
                chunk_id=chunk_id,
                document_id=metadata["document_id"],
                chunk_text=metadata["chunk_text"],
                score=score,
                metadata=metadata
            ))
        
        return results
    
    async def _keyword_search(
        self,
        query: str,
        candidates: List[SearchResult],
        top_k: int
    ) -> List[SearchResult]:
        """å…³é”®è¯æ£€ç´¢ï¼ˆBM25ï¼‰"""
        # åˆ†è¯
        query_tokens = list(jieba.cut_for_search(query))
        
        # è®¡ç®— BM25 åˆ†æ•°
        scored_results = []
        for result in candidates:
            doc_tokens = list(jieba.cut_for_search(result.chunk_text))
            score = self._bm25_score(query_tokens, doc_tokens, len(candidates))
            
            # å¤åˆ¶ç»“æœå¹¶æ›´æ–°åˆ†æ•°
            new_result = SearchResult(
                chunk_id=result.chunk_id,
                document_id=result.document_id,
                chunk_text=result.chunk_text,
                score=score,
                metadata=result.metadata,
                highlights=self._find_highlights(query_tokens, result.chunk_text)
            )
            scored_results.append(new_result)
        
        # æ’åº
        scored_results.sort(key=lambda x: x.score, reverse=True)
        return scored_results[:top_k]
    
    def _bm25_score(
        self,
        query_tokens: List[str],
        doc_tokens: List[str],
        corpus_size: int,
        k1: float = 1.5,
        b: float = 0.75
    ) -> float:
        """BM25 æ‰“åˆ†"""
        doc_len = len(doc_tokens)
        avg_doc_len = 100  # å‡è®¾å¹³å‡æ–‡æ¡£é•¿åº¦
        
        doc_freq = Counter(doc_tokens)
        score = 0.0
        
        for token in query_tokens:
            if token in doc_freq:
                tf = doc_freq[token]
                idf = math.log((corpus_size - 1 + 0.5) / (1 + 0.5))
                
                score += idf * (tf * (k1 + 1)) / (
                    tf + k1 * (1 - b + b * (doc_len / avg_doc_len))
                )
        
        return score
    
    def _fuse_results(
        self,
        semantic_results: List[SearchResult],
        keyword_results: List[SearchResult],
        top_k: int
    ) -> List[SearchResult]:
        """èåˆç»“æœï¼ˆRRF - Reciprocal Rank Fusionï¼‰"""
        # RRF å…¬å¼: score = sum(1 / (k + rank))
        k = 60  # RRF å¸¸æ•°
        
        scores = {}
        
        # è¯­ä¹‰æ£€ç´¢åˆ†æ•°
        for rank, result in enumerate(semantic_results, 1):
            chunk_id = result.chunk_id
            if chunk_id not in scores:
                scores[chunk_id] = {
                    "result": result,
                    "score": 0.0
                }
            scores[chunk_id]["score"] += self.semantic_weight / (k + rank)
        
        # å…³é”®è¯æ£€ç´¢åˆ†æ•°
        for rank, result in enumerate(keyword_results, 1):
            chunk_id = result.chunk_id
            if chunk_id not in scores:
                scores[chunk_id] = {
                    "result": result,
                    "score": 0.0
                }
            scores[chunk_id]["score"] += self.keyword_weight / (k + rank)
            
            # åˆå¹¶ highlights
            if result.highlights:
                scores[chunk_id]["result"].highlights = result.highlights
        
        # æ’åº
        sorted_items = sorted(
            scores.values(),
            key=lambda x: x["score"],
            reverse=True
        )
        
        # è¿”å› top_k
        fused_results = []
        for item in sorted_items[:top_k]:
            result = item["result"]
            result.score = item["score"]  # æ›´æ–°ä¸ºèåˆåˆ†æ•°
            fused_results.append(result)
        
        return fused_results
    
    def _find_highlights(self, query_tokens: List[str], text: str, window: int = 50) -> List[str]:
        """æŸ¥æ‰¾é«˜äº®ç‰‡æ®µ"""
        highlights = []
        text_lower = text.lower()
        
        for token in query_tokens:
            pos = text_lower.find(token.lower())
            if pos != -1:
                start = max(0, pos - window)
                end = min(len(text), pos + len(token) + window)
                highlight = text[start:end]
                if start > 0:
                    highlight = "..." + highlight
                if end < len(text):
                    highlight = highlight + "..."
                highlights.append(highlight)
        
        return highlights[:3]  # æœ€å¤šè¿”å› 3 ä¸ªé«˜äº®
```

### 4.2 é‡æ’åº

```python
# src/core/rag/reranker.py
from typing import List
from sentence_transformers import CrossEncoder
from models.domain import SearchResult
from utils.logging import get_logger

logger = get_logger(__name__)


class Reranker:
    """é‡æ’åºå™¨"""
    
    def __init__(self, model_name: str = "BAAI/bge-reranker-large"):
        """åˆå§‹åŒ–"""
        logger.info(f"Loading reranker model: {model_name}")
        self.model = CrossEncoder(model_name)
        logger.info("Reranker model loaded")
    
    def rerank(
        self,
        query: str,
        results: List[SearchResult],
        top_k: int = None
    ) -> List[SearchResult]:
        """é‡æ’åº
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            results: æœç´¢ç»“æœ
            top_k: è¿”å›æ•°é‡
            
        Returns:
            é‡æ’åºåçš„ç»“æœ
        """
        if not results:
            return results
        
        logger.info(f"Reranking {len(results)} results...")
        
        # å‡†å¤‡è¾“å…¥
        pairs = [[query, result.chunk_text] for result in results]
        
        # è®¡ç®—ç›¸å…³æ€§åˆ†æ•°
        scores = self.model.predict(pairs)
        
        # æ›´æ–°åˆ†æ•°å¹¶æ’åº
        for result, score in zip(results, scores):
            result.score = float(score)
        
        results.sort(key=lambda x: x.score, reverse=True)
        
        if top_k:
            results = results[:top_k]
        
        logger.info(f"Reranking completed, top score: {results[0].score:.4f}")
        return results
```

---

## äº”ã€äº‹ä»¶é©±åŠ¨ç´¢å¼•

### 5.1 äº‹ä»¶ç›‘å¬

```python
# src/core/rag/event_handlers.py
from typing import Dict, Any
from core.rag.knowledge_base import KnowledgeBaseManager
from models.domain import Document
from utils.logging import get_logger

logger = get_logger(__name__)


class RAGIndexEventHandler:
    """RAG ç´¢å¼•äº‹ä»¶å¤„ç†å™¨"""
    
    def __init__(self, kb_manager: KnowledgeBaseManager):
        """åˆå§‹åŒ–"""
        self.kb_manager = kb_manager
    
    async def handle_document_created(self, event_data: Dict[str, Any]):
        """å¤„ç†æ–‡æ¡£åˆ›å»ºäº‹ä»¶"""
        logger.info(f"Handling document created event: {event_data.get('document_id')}")
        
        # æ„å»º Document
        document = Document(
            id=event_data["document_id"],
            user_id=event_data["user_id"],
            project_id=event_data["project_id"],
            content=event_data["content"],
            content_type=event_data["content_type"],
            metadata=event_data.get("metadata", {})
        )
        
        # ç´¢å¼•æ–‡æ¡£
        try:
            await self.kb_manager.index_document(document)
            logger.info(f"Document indexed: {document.id}")
        except Exception as e:
            logger.error(f"Failed to index document {document.id}: {e}")
    
    async def handle_document_updated(self, event_data: Dict[str, Any]):
        """å¤„ç†æ–‡æ¡£æ›´æ–°äº‹ä»¶"""
        logger.info(f"Handling document updated event: {event_data.get('document_id')}")
        
        document_id = event_data["document_id"]
        
        # åˆ é™¤æ—§ç´¢å¼•
        try:
            await self.kb_manager.delete_document(document_id)
        except Exception as e:
            logger.error(f"Failed to delete old index for {document_id}: {e}")
        
        # é‡æ–°ç´¢å¼•
        document = Document(
            id=document_id,
            user_id=event_data["user_id"],
            project_id=event_data["project_id"],
            content=event_data["content"],
            content_type=event_data["content_type"],
            metadata=event_data.get("metadata", {})
        )
        
        try:
            await self.kb_manager.index_document(document)
            logger.info(f"Document re-indexed: {document_id}")
        except Exception as e:
            logger.error(f"Failed to re-index document {document_id}: {e}")
    
    async def handle_document_deleted(self, event_data: Dict[str, Any]):
        """å¤„ç†æ–‡æ¡£åˆ é™¤äº‹ä»¶"""
        document_id = event_data["document_id"]
        logger.info(f"Handling document deleted event: {document_id}")
        
        try:
            await self.kb_manager.delete_document(document_id)
            logger.info(f"Document index deleted: {document_id}")
        except Exception as e:
            logger.error(f"Failed to delete document index {document_id}: {e}")
```

---

## å…­ã€æ€§èƒ½ä¼˜åŒ–

### 6.1 æ‰¹é‡å¤„ç†

- **æ‰¹é‡å‘é‡åŒ–**ï¼šbatch_size=32ï¼Œæå‡ 10 å€æ•ˆç‡
- **æ‰¹é‡æ’å…¥**ï¼šå‡å°‘ç½‘ç»œå¾€è¿”æ¬¡æ•°
- **å¼‚æ­¥å¤„ç†**ï¼šä½¿ç”¨ asyncio å¹¶å‘å¤„ç†

### 6.2 ç´¢å¼•ä¼˜åŒ–

- **HNSW ç´¢å¼•**ï¼šå¿«é€Ÿè¿‘ä¼¼æœç´¢ï¼Œtrade-off ç²¾åº¦å’Œé€Ÿåº¦
- **å‚æ•°è°ƒä¼˜**ï¼šM=16, efConstruction=128, ef=64

### 6.3 ç¼“å­˜ç­–ç•¥

```python
# æŸ¥è¯¢ç¼“å­˜
from functools import lru_cache

@lru_cache(maxsize=1000)
def cached_search(query: str, user_id: str, project_id: str, top_k: int):
    # ç¼“å­˜çƒ­é—¨æŸ¥è¯¢
    pass
```

---

## ä¸ƒã€ç›‘æ§å’ŒæŒ‡æ ‡

### 7.1 å…³é”®æŒ‡æ ‡

- **æ£€ç´¢å»¶è¿Ÿ**ï¼šP50, P95, P99
- **æ£€ç´¢å‡†ç¡®ç‡**ï¼šMRR, NDCG
- **ç´¢å¼•é€Ÿåº¦**ï¼šæ–‡æ¡£/ç§’
- **å­˜å‚¨ç©ºé—´**ï¼šå‘é‡æ•°é‡ã€ç£ç›˜å ç”¨

### 7.2 Prometheus æŒ‡æ ‡

```python
# metrics
rag_search_duration = Histogram('rag_search_duration_seconds')
rag_index_duration = Histogram('rag_index_duration_seconds')
rag_search_results_count = Histogram('rag_search_results_count')
```

---

## å…«ã€æµ‹è¯•ç­–ç•¥

### 8.1 å‡†ç¡®ç‡æµ‹è¯•

- æ„å»ºæµ‹è¯•é›†ï¼šquery-document pairs
- è®¡ç®— MRRã€NDCG
- A/B æµ‹è¯•ä¸åŒæ£€ç´¢ç­–ç•¥

### 8.2 æ€§èƒ½æµ‹è¯•

- å¹¶å‘æ£€ç´¢å‹æµ‹
- å¤§è§„æ¨¡ç´¢å¼•æµ‹è¯•
- é•¿æ—¶é—´ç¨³å®šæ€§æµ‹è¯•

---

## ä¹ã€éƒ¨ç½²å’Œé…ç½®

### 9.1 Milvus éƒ¨ç½²

```yaml
# docker-compose.yaml
services:
  milvus:
    image: milvusdb/milvus:latest
    ports:
      - "19530:19530"
    environment:
      - ETCD_USE_EMBED=true
    volumes:
      - milvus-data:/var/lib/milvus
```

### 9.2 é…ç½®ç¤ºä¾‹

```yaml
rag:
  vectorization:
    model: "BAAI/bge-large-zh-v1.5"
    batch_size: 32
    device: "cuda"
  
  chunking:
    chunk_size: 500
    chunk_overlap: 50
  
  search:
    top_k: 10
    semantic_weight: 0.7
    keyword_weight: 0.3
    enable_rerank: true
  
  milvus:
    host: "localhost"
    port: 19530
```

---

## åã€æ€»ç»“

RAG æ£€ç´¢å¢å¼ºç³»ç»Ÿæ˜¯ AI èƒ½åŠ›çš„æ ¸å¿ƒåŸºç¡€è®¾æ–½ï¼Œé€šè¿‡å‘é‡æ£€ç´¢å’Œæ··åˆæœç´¢æŠ€æœ¯ï¼Œä¸º Agent æä¾›ç²¾å‡†çš„ä¸Šä¸‹æ–‡å¢å¼ºèƒ½åŠ›ã€‚

**å…³é”®ç‰¹æ€§**ï¼š
- âœ… é«˜ç²¾åº¦æ··åˆæ£€ç´¢ï¼ˆè¯­ä¹‰ + å…³é”®è¯ï¼‰
- âœ… æ™ºèƒ½åˆ†å—å’Œå‘é‡åŒ–
- âœ… äº‹ä»¶é©±åŠ¨å®æ—¶ç´¢å¼•
- âœ… é‡æ’åºä¼˜åŒ–
- âœ… é«˜æ€§èƒ½å’Œå¯æ‰©å±•æ€§

**åç»­å·¥ä½œ**ï¼š
- ä¸ Agent å·¥ä½œæµé›†æˆ
- RAG Tool å°è£…
- æ€§èƒ½ä¼˜åŒ–å’Œæµ‹è¯•

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**åˆ›å»ºæ—¶é—´**: 2025-10-27
**ç»´æŠ¤è€…**: AIæ¶æ„ç»„
