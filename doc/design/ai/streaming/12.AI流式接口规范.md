# AI流式接口规范

> **文档版本**: v1.0  
> **创建时间**: 2025-10-21  
> **实施状态**: 设计阶段

## 📋 文档概述

本文档规定AI生成接口必须使用流式响应，以降低用户感知延迟，提升用户体验。

## 🎯 设计目标

1. **强制流式**：所有AI生成接口必须支持流式响应
2. **降低延迟感知**：用户即时看到生成过程
3. **前端友好**：提供标准的SSE/WebSocket接口
4. **性能优化**：合理的chunk大小和推送频率

---

## 一、强制流式接口

### 1.1 接口规范

```go
// ❌ 禁止：非流式接口（已废弃）
func GenerateText(ctx context.Context, req *GenerateRequest) (*GenerateResponse, error)

// ✅ 推荐：流式接口（主接口）
func GenerateTextStream(ctx context.Context, req *GenerateRequest) (<-chan *StreamChunk, error)
```

### 1.2 API层实现

```go
package api

func (api *AIApi) GenerateText(c *gin.Context) {
    var req ai.GenerateRequest
    if err := c.ShouldBindJSON(&req); err != nil {
        response.Error(c, http.StatusBadRequest, "参数错误", err.Error())
        return
    }
    
    // 强制使用流式接口
    api.GenerateTextStream(c, &req)
}
```

---

## 二、SSE实现规范

### 2.1 SSE响应头设置

```go
func (api *AIApi) GenerateTextStream(c *gin.Context, req *ai.GenerateRequest) {
    // 1. 设置SSE响应头
    c.Header("Content-Type", "text/event-stream")
    c.Header("Cache-Control", "no-cache")
    c.Header("Connection", "keep-alive")
    c.Header("X-Accel-Buffering", "no")  // 禁用Nginx缓冲
    c.Header("Access-Control-Allow-Origin", "*")
    
    // 2. 获取流式channel
    chunkChan, err := api.proxyService.GenerateTextStream(c.Request.Context(), req)
    if err != nil {
        api.handleError(c, err)
        return
    }
    
    // 3. 流式推送
    c.Stream(func(w io.Writer) bool {
        select {
        case <-c.Request.Context().Done():
            // 客户端断开连接
            return false
            
        case chunk, ok := <-chunkChan:
            if !ok {
                // channel关闭
                return false
            }
            
            if chunk.Error != "" {
                // 发送错误事件
                c.SSEvent("error", gin.H{
                    "error": chunk.Error,
                })
                return false
            }
            
            // 发送数据事件
            c.SSEvent("message", gin.H{
                "delta":   chunk.Delta,
                "isFinal": chunk.IsFinal,
            })
            
            return !chunk.IsFinal
        }
    })
}
```

### 2.2 SSE事件格式

```
event: message
data: {"delta":"这是","isFinal":false}

event: message
data: {"delta":"一段","isFinal":false}

event: message
data: {"delta":"生成的","isFinal":false}

event: message
data: {"delta":"文本。","isFinal":true}
```

---

## 三、前端集成规范

### 3.1 EventSource使用

```javascript
// 前端：使用EventSource接收SSE
function generateTextStream(prompt) {
    const eventSource = new EventSource(`/api/v1/ai/generate?stream=true`, {
        method: 'POST',
        headers: {
            'Content-Type': 'application/json',
            'Authorization': `Bearer ${token}`
        },
        body: JSON.stringify({ prompt })
    });
    
    let fullText = '';
    
    eventSource.addEventListener('message', (event) => {
        const data = JSON.parse(event.data);
        
        fullText += data.delta;
        
        // 实时更新UI
        updateTextDisplay(fullText);
        
        if (data.isFinal) {
            eventSource.close();
            onComplete(fullText);
        }
    });
    
    eventSource.addEventListener('error', (event) => {
        const error = JSON.parse(event.data);
        console.error('生成失败:', error.error);
        eventSource.close();
        onError(error.error);
    });
    
    // 超时控制
    setTimeout(() => {
        if (eventSource.readyState !== EventSource.CLOSED) {
            eventSource.close();
            onError('生成超时');
        }
    }, 120000); // 2分钟超时
}
```

### 3.2 Fetch API流式处理（备选）

```javascript
async function generateTextStreamFetch(prompt) {
    const response = await fetch('/api/v1/ai/generate', {
        method: 'POST',
        headers: {
            'Content-Type': 'application/json',
            'Authorization': `Bearer ${token}`
        },
        body: JSON.stringify({ prompt, stream: true })
    });
    
    const reader = response.body.getReader();
    const decoder = new TextDecoder();
    let fullText = '';
    
    while (true) {
        const { done, value } = await reader.read();
        
        if (done) break;
        
        const chunk = decoder.decode(value);
        const lines = chunk.split('\n\n');
        
        for (const line of lines) {
            if (line.startsWith('data: ')) {
                const data = JSON.parse(line.substring(6));
                fullText += data.delta;
                updateTextDisplay(fullText);
            }
        }
    }
    
    return fullText;
}
```

---

## 四、性能优化

### 4.1 Chunk大小控制

```go
// Python端：控制chunk大小
async def generate_stream(prompt: str, model: str):
    """流式生成"""
    buffer = ""
    chunk_size = 5  # 每5个字符发送一次
    
    async for token in llm_client.generate_stream(prompt, model):
        buffer += token
        
        if len(buffer) >= chunk_size:
            yield StreamChunk(delta=buffer, is_final=False)
            buffer = ""
    
    # 发送剩余内容
    if buffer:
        yield StreamChunk(delta=buffer, is_final=True)
```

### 4.2 推送频率控制

```go
// Go端：控制推送频率
func (s *AIProxyServiceImpl) handleStreamWithThrottle(
    ctx context.Context,
    stream pb.AIAgentService_GenerateTextStreamClient,
    chunkChan chan<- *StreamChunk,
) {
    defer close(chunkChan)
    
    ticker := time.NewTicker(50 * time.Millisecond) // 最快每50ms推送一次
    defer ticker.Stop()
    
    buffer := ""
    
    for {
        select {
        case <-ctx.Done():
            return
            
        case <-ticker.C:
            if buffer != "" {
                chunkChan <- &StreamChunk{Delta: buffer, IsFinal: false}
                buffer = ""
            }
            
        default:
            chunk, err := stream.Recv()
            if err == io.EOF {
                if buffer != "" {
                    chunkChan <- &StreamChunk{Delta: buffer, IsFinal: true}
                }
                return
            }
            if err != nil {
                chunkChan <- &StreamChunk{Error: err.Error()}
                return
            }
            
            buffer += chunk.Delta
        }
    }
}
```

### 4.3 内存控制

```go
// 限制channel缓冲区大小
chunkChan := make(chan *StreamChunk, 100) // 最多缓冲100个chunk

// 超时控制
ctx, cancel := context.WithTimeout(ctx, 2*time.Minute)
defer cancel()
```

---

## 五、错误处理

### 5.1 流式错误响应

```go
// 发送错误事件
c.SSEvent("error", gin.H{
    "error": "生成失败",
    "code":  "GENERATION_ERROR",
    "details": err.Error(),
})
```

### 5.2 连接保活

```go
// 定期发送心跳
ticker := time.NewTicker(15 * time.Second)
defer ticker.Stop()

for {
    select {
    case <-ticker.C:
        c.SSEvent("ping", gin.H{"timestamp": time.Now().Unix()})
        
    case chunk := <-chunkChan:
        // 处理chunk
    }
}
```

---

## 六、测试规范

### 6.1 性能测试

```go
func BenchmarkGenerateTextStream(b *testing.B) {
    for i := 0; i < b.N; i++ {
        req := &ai.GenerateRequest{
            Prompt: "写一段500字的故事",
            Model:  "gpt-4",
        }
        
        chunkChan, err := proxyService.GenerateTextStream(context.Background(), req)
        if err != nil {
            b.Fatal(err)
        }
        
        for chunk := range chunkChan {
            if chunk.Error != "" {
                b.Fatal(chunk.Error)
            }
        }
    }
}
```

### 6.2 压力测试

```bash
# 使用k6进行压力测试
import http from 'k6/http';
import { check } from 'k6';

export default function() {
    const payload = JSON.stringify({
        prompt: '写一段故事',
        stream: true
    });
    
    const params = {
        headers: {
            'Content-Type': 'application/json',
            'Authorization': `Bearer ${__ENV.TOKEN}`
        }
    };
    
    const res = http.post('http://localhost:8080/api/v1/ai/generate', payload, params);
    
    check(res, {
        'status is 200': (r) => r.status === 200,
        'is streaming': (r) => r.headers['Content-Type'] === 'text/event-stream'
    });
}
```

---

## 七、实施建议

### 7.1 迁移计划

1. **Week 1**: 实现SSE基础框架
2. **Week 2**: 改造所有AI接口为流式
3. **Week 3**: 前端集成和测试
4. **Week 4**: 性能优化和监控

### 7.2 监控指标

| 指标 | 目标 | 告警阈值 |
|------|------|---------|
| 首个chunk延迟 | <500ms | >1s |
| 平均chunk间隔 | <100ms | >200ms |
| 流式连接成功率 | >99% | <95% |
| 并发连接数 | 支持1000+ | >1500 |

---

**文档版本**: v1.0  
**创建时间**: 2025-10-21  
**负责人**: AI团队  
**审核状态**: 待评审

