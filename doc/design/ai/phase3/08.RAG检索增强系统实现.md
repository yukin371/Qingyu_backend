# RAG 检索增强系统实现

> **文档版本**: v1.0
> **创建时间**: 2025-10-27
> **实施状态**: 设计阶段
> **负责人**: AI架构组

---

## 📋 文档概述

本文档详细设计青羽平台的 RAG（Retrieval Augmented Generation）检索增强系统，用于智能写作续写、设定问答、阅读助手等场景。基于向量检索和混合搜索技术，为 AI Agent 提供精准的上下文增强能力。

**适用范围**：
- 向量化引擎实现
- 知识库管理
- 混合检索策略
- 与 Agent 的集成

---

## 🎯 设计目标

### 核心目标

1. **高精度检索**：语义相似度 + 关键词匹配，检索准确率 ≥85%
2. **低延迟响应**：P95 延迟 < 200ms
3. **大规模支持**：支持百万级文档索引
4. **灵活扩展**：易于添加新的检索策略和重排序算法
5. **事件驱动索引**：实时更新索引，保持数据一致性

### 非目标

- ❌ 不实现分布式向量数据库（使用 Milvus）
- ❌ 不训练自定义 Embedding 模型（使用预训练模型）

---

## 一、系统架构

### 1.1 整体架构图

```
┌──────────────────────────────────────────────────────────────┐
│                   RAG System Architecture                     │
│                                                                │
│  ┌──────────────────────────────────────────────────────┐   │
│  │         Vectorization Engine (向量化引擎)            │   │
│  │  ┌────────────────────────────────────────────────┐  │   │
│  │  │  Text Preprocessing                            │  │   │
│  │  │  - HTML/Markdown 清理                          │  │   │
│  │  │  - 空白字符标准化                              │  │   │
│  │  │  - 特殊字符处理                                │  │   │
│  │  └────────────┬───────────────────────────────────┘  │   │
│  │               │                                       │   │
│  │  ┌────────────▼───────────────────────────────────┐  │   │
│  │  │  Chunk Splitter (智能分块)                    │  │   │
│  │  │  - 按段落分割                                  │  │   │
│  │  │  - 语义边界检测                                │  │   │
│  │  │  - 重叠分块策略                                │  │   │
│  │  └────────────┬───────────────────────────────────┘  │   │
│  │               │                                       │   │
│  │  ┌────────────▼───────────────────────────────────┐  │   │
│  │  │  Embedding Model (BGE-large-zh-v1.5)          │  │   │
│  │  │  - Batch Processing                            │  │   │
│  │  │  - 1024 维向量                                 │  │   │
│  │  └────────────────────────────────────────────────┘  │   │
│  └──────────────────┬───────────────────────────────────┘   │
│                     │                                        │
│  ┌──────────────────▼───────────────────────────────────┐   │
│  │        Knowledge Base Manager (知识库管理)          │   │
│  │  ┌────────────────────────────────────────────────┐  │   │
│  │  │  Document Model                                │  │   │
│  │  │  - document_id, user_id, project_id            │  │   │
│  │  │  - content, content_type, metadata             │  │   │
│  │  └────────────────────────────────────────────────┘  │   │
│  │  ┌────────────────────────────────────────────────┐  │   │
│  │  │  VectorDocument Model                          │  │   │
│  │  │  - chunk_id, document_id, chunk_index          │  │   │
│  │  │  - chunk_text, embedding, metadata             │  │   │
│  │  └────────────────────────────────────────────────┘  │   │
│  └──────────────────┬───────────────────────────────────┘   │
│                     │                                        │
│  ┌──────────────────▼───────────────────────────────────┐   │
│  │          Milvus Vector Database                       │   │
│  │  Collection: user_private_kb                          │   │
│  │  Collection: platform_kb                              │   │
│  │  Index: HNSW (M=16, efConstruction=128)              │   │
│  └──────────────────┬───────────────────────────────────┘   │
│                     │                                        │
│  ┌──────────────────▼───────────────────────────────────┐   │
│  │        Retrieval Engine (检索引擎)                   │   │
│  │  ┌────────────────────────────────────────────────┐  │   │
│  │  │  Semantic Search (向量相似度检索)             │  │   │
│  │  │  - HNSW 索引快速搜索                          │  │   │
│  │  │  - 余弦相似度计算                             │  │   │
│  │  └────────────────────────────────────────────────┘  │   │
│  │  ┌────────────────────────────────────────────────┐  │   │
│  │  │  Keyword Search (BM25 关键词检索)            │  │   │
│  │  │  - 基于 jieba 分词                            │  │   │
│  │  │  - TF-IDF 权重                                │  │   │
│  │  └────────────────────────────────────────────────┘  │   │
│  │  ┌────────────────────────────────────────────────┐  │   │
│  │  │  Hybrid Search (混合检索)                     │  │   │
│  │  │  - RRF (Reciprocal Rank Fusion)               │  │   │
│  │  │  - 加权融合策略                               │  │   │
│  │  └────────────────────────────────────────────────┘  │   │
│  │  ┌────────────────────────────────────────────────┐  │   │
│  │  │  Reranker (重排序)                            │  │   │
│  │  │  - BGE-reranker-large                         │  │   │
│  │  │  - Cross-encoder 精准打分                      │  │   │
│  │  └────────────────────────────────────────────────┘  │   │
│  └────────────────────────────────────────────────────────┘   │
└──────────────────────────────────────────────────────────────┘
```

### 1.2 核心组件

| 组件 | 职责 | 技术选型 |
|------|------|---------|
| **Vectorization Engine** | 文本预处理和向量化 | sentence-transformers |
| **Embedding Model** | 文本向量编码 | BGE-large-zh-v1.5 |
| **Vector Database** | 向量存储和检索 | Milvus |
| **Keyword Search** | 关键词匹配 | jieba + BM25 |
| **Reranker** | 结果重排序 | BGE-reranker-large |

---

## 二、向量化引擎实现

### 2.1 文本预处理

```python
# src/core/rag/preprocessing.py
import re
from typing import List
from bs4 import BeautifulSoup
import markdown


class TextPreprocessor:
    """文本预处理器"""
    
    def preprocess(self, text: str, content_type: str = "text") -> str:
        """预处理文本
        
        Args:
            text: 原始文本
            content_type: 内容类型 (text, html, markdown)
            
        Returns:
            处理后的文本
        """
        # 1. 根据类型清理
        if content_type == "html":
            text = self._clean_html(text)
        elif content_type == "markdown":
            text = self._clean_markdown(text)
        
        # 2. 标准化空白字符
        text = self._normalize_whitespace(text)
        
        # 3. 移除特殊字符（保留中英文和标点）
        text = self._clean_special_chars(text)
        
        return text.strip()
    
    def _clean_html(self, html: str) -> str:
        """清理 HTML 标签"""
        soup = BeautifulSoup(html, 'html.parser')
        # 移除 script 和 style 标签
        for script in soup(["script", "style"]):
            script.decompose()
        return soup.get_text()
    
    def _clean_markdown(self, md: str) -> str:
        """清理 Markdown 格式"""
        # 转换为 HTML 再提取文本
        html = markdown.markdown(md)
        return self._clean_html(html)
    
    def _normalize_whitespace(self, text: str) -> str:
        """标准化空白字符"""
        # 统一换行符
        text = text.replace('\r\n', '\n').replace('\r', '\n')
        # 移除多余空白
        text = re.sub(r'[ \t]+', ' ', text)
        # 移除多余换行
        text = re.sub(r'\n{3,}', '\n\n', text)
        return text
    
    def _clean_special_chars(self, text: str) -> str:
        """清理特殊字符"""
        # 保留中文、英文、数字、基本标点
        text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9\s\.,;:!?()（）【】「」""''…—\-]', '', text)
        return text
```

### 2.2 智能分块策略

```python
# src/core/rag/chunking.py
from typing import List, Tuple
import jieba


class ChunkSplitter:
    """智能分块器"""
    
    def __init__(
        self,
        chunk_size: int = 500,
        chunk_overlap: int = 50,
        separators: List[str] = None
    ):
        """初始化
        
        Args:
            chunk_size: 块大小（字符数）
            chunk_overlap: 重叠大小
            separators: 分隔符列表
        """
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.separators = separators or ['\n\n', '\n', '。', '！', '？', ';', '；']
    
    def split(self, text: str) -> List[Tuple[str, int]]:
        """分块
        
        Args:
            text: 文本
            
        Returns:
            [(chunk_text, start_offset), ...]
        """
        chunks = []
        
        # 1. 按段落分割
        paragraphs = self._split_by_separators(text)
        
        # 2. 组装 chunks
        current_chunk = ""
        current_offset = 0
        
        for para, offset in paragraphs:
            # 如果当前块加上新段落不超过限制，直接添加
            if len(current_chunk) + len(para) <= self.chunk_size:
                current_chunk += para + "\n\n"
            else:
                # 保存当前块
                if current_chunk:
                    chunks.append((current_chunk.strip(), current_offset))
                
                # 如果单个段落超过 chunk_size，需要进一步切分
                if len(para) > self.chunk_size:
                    sub_chunks = self._split_long_paragraph(para)
                    chunks.extend([(c, offset) for c in sub_chunks])
                    current_chunk = ""
                    current_offset = offset + len(para)
                else:
                    current_chunk = para + "\n\n"
                    current_offset = offset
        
        # 添加最后一个块
        if current_chunk:
            chunks.append((current_chunk.strip(), current_offset))
        
        # 3. 添加重叠
        chunks = self._add_overlap(chunks)
        
        return chunks
    
    def _split_by_separators(self, text: str) -> List[Tuple[str, int]]:
        """按分隔符分割"""
        parts = []
        current = ""
        offset = 0
        
        for separator in self.separators:
            if separator in text:
                segments = text.split(separator)
                for i, seg in enumerate(segments):
                    if i < len(segments) - 1:
                        parts.append((seg + separator, offset))
                        offset += len(seg) + len(separator)
                    else:
                        parts.append((seg, offset))
                break
        
        if not parts:
            parts = [(text, 0)]
        
        return parts
    
    def _split_long_paragraph(self, para: str) -> List[str]:
        """分割长段落（按句子）"""
        # 使用 jieba 分句
        sentences = []
        current = ""
        
        for char in para:
            current += char
            if char in ['。', '！', '？', '\n']:
                sentences.append(current)
                current = ""
        
        if current:
            sentences.append(current)
        
        # 组装成 chunk_size 大小的块
        chunks = []
        current_chunk = ""
        
        for sentence in sentences:
            if len(current_chunk) + len(sentence) <= self.chunk_size:
                current_chunk += sentence
            else:
                if current_chunk:
                    chunks.append(current_chunk)
                current_chunk = sentence
        
        if current_chunk:
            chunks.append(current_chunk)
        
        return chunks
    
    def _add_overlap(self, chunks: List[Tuple[str, int]]) -> List[Tuple[str, int]]:
        """添加重叠"""
        if len(chunks) <= 1:
            return chunks
        
        overlapped = []
        for i, (chunk, offset) in enumerate(chunks):
            if i > 0:
                # 添加前一个块的尾部
                prev_chunk = chunks[i-1][0]
                overlap_text = prev_chunk[-self.chunk_overlap:]
                chunk = overlap_text + chunk
            
            overlapped.append((chunk, offset))
        
        return overlapped
```

### 2.3 向量编码

```python
# src/core/rag/vectorizer.py
from typing import List
import numpy as np
from sentence_transformers import SentenceTransformer
from utils.logging import get_logger

logger = get_logger(__name__)


class EmbeddingEngine:
    """向量编码引擎"""
    
    def __init__(
        self,
        model_name: str = "BAAI/bge-large-zh-v1.5",
        batch_size: int = 32,
        device: str = "cuda"
    ):
        """初始化
        
        Args:
            model_name: 模型名称
            batch_size: 批处理大小
            device: 设备 (cuda/cpu)
        """
        self.model_name = model_name
        self.batch_size = batch_size
        self.device = device
        
        logger.info(f"Loading embedding model: {model_name}")
        self.model = SentenceTransformer(model_name, device=device)
        logger.info("Embedding model loaded")
    
    def encode(
        self,
        texts: List[str],
        show_progress: bool = False
    ) -> np.ndarray:
        """批量编码
        
        Args:
            texts: 文本列表
            show_progress: 是否显示进度
            
        Returns:
            向量数组 (n, dim)
        """
        logger.info(f"Encoding {len(texts)} texts...")
        
        embeddings = self.model.encode(
            texts,
            batch_size=self.batch_size,
            show_progress_bar=show_progress,
            normalize_embeddings=True  # 归一化，便于余弦相似度计算
        )
        
        logger.info(f"Encoding completed, shape: {embeddings.shape}")
        return embeddings
    
    def encode_query(self, query: str) -> np.ndarray:
        """编码查询（添加指令前缀）"""
        # BGE 模型推荐为查询添加指令
        query_with_instruction = f"为这个句子生成表示以用于检索相关文章：{query}"
        return self.encode([query_with_instruction])[0]
```

---

## 三、知识库管理

### 3.1 数据模型

```python
# src/models/domain.py
from dataclasses import dataclass, field
from datetime import datetime
from typing import List, Dict, Any, Optional


@dataclass
class Document:
    """文档模型"""
    id: str
    user_id: str
    project_id: str
    content: str
    content_type: str  # 'chapter', 'character', 'setting', 'outline', 'timeline'
    metadata: Dict[str, Any] = field(default_factory=dict)
    created_at: datetime = field(default_factory=datetime.now)
    updated_at: datetime = field(default_factory=datetime.now)


@dataclass
class VectorDocument:
    """向量文档模型"""
    id: str  # chunk_id
    document_id: str
    chunk_index: int
    chunk_text: str
    embedding: List[float]
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    def to_milvus_entity(self) -> Dict[str, Any]:
        """转换为 Milvus 实体"""
        return {
            "id": self.id,
            "document_id": self.document_id,
            "chunk_index": self.chunk_index,
            "chunk_text": self.chunk_text,
            "embedding": self.embedding,
            "user_id": self.metadata.get("user_id", ""),
            "project_id": self.metadata.get("project_id", ""),
            "content_type": self.metadata.get("content_type", ""),
            "metadata_json": json.dumps(self.metadata)
        }
```

### 3.2 Milvus 客户端

```python
# src/infrastructure/vector_db/milvus_client.py
from pymilvus import (
    connections,
    Collection,
    CollectionSchema,
    FieldSchema,
    DataType,
    utility
)
from typing import List, Dict, Any, Tuple
from utils.logging import get_logger

logger = get_logger(__name__)


class MilvusClient:
    """Milvus 客户端"""
    
    COLLECTION_NAME = "qingyu_kb"
    VECTOR_DIM = 1024  # BGE-large-zh-v1.5 的维度
    
    def __init__(
        self,
        host: str = "localhost",
        port: int = 19530,
        user: str = "",
        password: str = ""
    ):
        """初始化"""
        self.host = host
        self.port = port
        
        # 连接
        logger.info(f"Connecting to Milvus at {host}:{port}")
        connections.connect(
            alias="default",
            host=host,
            port=port,
            user=user,
            password=password
        )
        
        # 创建 collection
        self._create_collection_if_not_exists()
        
        # 加载 collection
        self.collection = Collection(self.COLLECTION_NAME)
        self.collection.load()
        
        logger.info(f"Milvus client initialized, collection: {self.COLLECTION_NAME}")
    
    def _create_collection_if_not_exists(self):
        """创建 collection（如果不存在）"""
        if utility.has_collection(self.COLLECTION_NAME):
            logger.info(f"Collection {self.COLLECTION_NAME} already exists")
            return
        
        # 定义 schema
        fields = [
            FieldSchema(name="id", dtype=DataType.VARCHAR, is_primary=True, max_length=100),
            FieldSchema(name="document_id", dtype=DataType.VARCHAR, max_length=100),
            FieldSchema(name="chunk_index", dtype=DataType.INT64),
            FieldSchema(name="chunk_text", dtype=DataType.VARCHAR, max_length=10000),
            FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=self.VECTOR_DIM),
            FieldSchema(name="user_id", dtype=DataType.VARCHAR, max_length=100),
            FieldSchema(name="project_id", dtype=DataType.VARCHAR, max_length=100),
            FieldSchema(name="content_type", dtype=DataType.VARCHAR, max_length=50),
            FieldSchema(name="metadata_json", dtype=DataType.VARCHAR, max_length=5000),
        ]
        
        schema = CollectionSchema(
            fields=fields,
            description="青羽知识库"
        )
        
        # 创建 collection
        collection = Collection(
            name=self.COLLECTION_NAME,
            schema=schema
        )
        
        # 创建索引
        index_params = {
            "metric_type": "COSINE",  # 余弦相似度
            "index_type": "HNSW",
            "params": {
                "M": 16,
                "efConstruction": 128
            }
        }
        
        collection.create_index(
            field_name="embedding",
            index_params=index_params
        )
        
        logger.info(f"Collection {self.COLLECTION_NAME} created with HNSW index")
    
    def insert(self, vector_docs: List[VectorDocument]) -> List[str]:
        """插入向量文档
        
        Args:
            vector_docs: 向量文档列表
            
        Returns:
            插入的 ID 列表
        """
        if not vector_docs:
            return []
        
        entities = [doc.to_milvus_entity() for doc in vector_docs]
        
        # 转换为列表格式
        data = [
            [e["id"] for e in entities],
            [e["document_id"] for e in entities],
            [e["chunk_index"] for e in entities],
            [e["chunk_text"] for e in entities],
            [e["embedding"] for e in entities],
            [e["user_id"] for e in entities],
            [e["project_id"] for e in entities],
            [e["content_type"] for e in entities],
            [e["metadata_json"] for e in entities],
        ]
        
        result = self.collection.insert(data)
        self.collection.flush()
        
        logger.info(f"Inserted {len(vector_docs)} vector documents")
        return result.primary_keys
    
    def search(
        self,
        query_vector: List[float],
        top_k: int = 10,
        filters: Dict[str, Any] = None
    ) -> List[Tuple[str, float, Dict[str, Any]]]:
        """搜索
        
        Args:
            query_vector: 查询向量
            top_k: 返回数量
            filters: 过滤条件 {"user_id": "xxx", "project_id": "yyy"}
            
        Returns:
            [(chunk_id, score, metadata), ...]
        """
        # 构建过滤表达式
        expr = self._build_filter_expr(filters)
        
        # 搜索
        search_params = {
            "metric_type": "COSINE",
            "params": {"ef": 64}
        }
        
        results = self.collection.search(
            data=[query_vector],
            anns_field="embedding",
            param=search_params,
            limit=top_k,
            expr=expr,
            output_fields=["document_id", "chunk_text", "user_id", "project_id", "content_type", "metadata_json"]
        )
        
        # 转换结果
        hits = []
        for hit in results[0]:
            hits.append((
                hit.id,
                hit.score,
                {
                    "document_id": hit.entity.get("document_id"),
                    "chunk_text": hit.entity.get("chunk_text"),
                    "user_id": hit.entity.get("user_id"),
                    "project_id": hit.entity.get("project_id"),
                    "content_type": hit.entity.get("content_type"),
                    "metadata": json.loads(hit.entity.get("metadata_json", "{}"))
                }
            ))
        
        logger.info(f"Search returned {len(hits)} results")
        return hits
    
    def delete_by_document_id(self, document_id: str):
        """删除文档的所有向量"""
        expr = f'document_id == "{document_id}"'
        self.collection.delete(expr)
        self.collection.flush()
        logger.info(f"Deleted vectors for document: {document_id}")
    
    def _build_filter_expr(self, filters: Dict[str, Any]) -> str:
        """构建过滤表达式"""
        if not filters:
            return ""
        
        conditions = []
        for key, value in filters.items():
            if isinstance(value, str):
                conditions.append(f'{key} == "{value}"')
            elif isinstance(value, (int, float)):
                conditions.append(f'{key} == {value}')
            elif isinstance(value, list):
                # IN 查询
                values_str = ", ".join([f'"{v}"' if isinstance(v, str) else str(v) for v in value])
                conditions.append(f'{key} in [{values_str}]')
        
        return " && ".join(conditions)
```

### 3.3 知识库管理器

```python
# src/core/rag/knowledge_base.py
from typing import List, Dict, Any
from models.domain import Document, VectorDocument
from core.rag.preprocessing import TextPreprocessor
from core.rag.chunking import ChunkSplitter
from core.rag.vectorizer import EmbeddingEngine
from infrastructure.vector_db.milvus_client import MilvusClient
from utils.logging import get_logger

logger = get_logger(__name__)


class KnowledgeBaseManager:
    """知识库管理器"""
    
    def __init__(
        self,
        vectorizer: EmbeddingEngine,
        vector_db: MilvusClient,
        preprocessor: TextPreprocessor = None,
        chunker: ChunkSplitter = None
    ):
        """初始化"""
        self.vectorizer = vectorizer
        self.vector_db = vector_db
        self.preprocessor = preprocessor or TextPreprocessor()
        self.chunker = chunker or ChunkSplitter(chunk_size=500, chunk_overlap=50)
    
    async def index_document(self, document: Document) -> int:
        """索引文档
        
        Args:
            document: 文档
            
        Returns:
            索引的块数量
        """
        logger.info(f"Indexing document: {document.id}")
        
        # 1. 预处理
        cleaned_text = self.preprocessor.preprocess(
            document.content,
            content_type=document.metadata.get("format", "text")
        )
        
        # 2. 分块
        chunks = self.chunker.split(cleaned_text)
        logger.info(f"Document split into {len(chunks)} chunks")
        
        # 3. 向量化
        chunk_texts = [chunk[0] for chunk in chunks]
        embeddings = self.vectorizer.encode(chunk_texts)
        
        # 4. 构建向量文档
        vector_docs = []
        for i, ((chunk_text, offset), embedding) in enumerate(zip(chunks, embeddings)):
            vector_doc = VectorDocument(
                id=f"{document.id}_{i}",
                document_id=document.id,
                chunk_index=i,
                chunk_text=chunk_text,
                embedding=embedding.tolist(),
                metadata={
                    "user_id": document.user_id,
                    "project_id": document.project_id,
                    "content_type": document.content_type,
                    "offset": offset,
                    **document.metadata
                }
            )
            vector_docs.append(vector_doc)
        
        # 5. 插入向量数据库
        self.vector_db.insert(vector_docs)
        
        logger.info(f"Document indexed: {document.id}, {len(vector_docs)} chunks")
        return len(vector_docs)
    
    async def delete_document(self, document_id: str):
        """删除文档索引"""
        logger.info(f"Deleting document index: {document_id}")
        self.vector_db.delete_by_document_id(document_id)
        logger.info(f"Document index deleted: {document_id}")
    
    async def batch_index(self, documents: List[Document]) -> Dict[str, int]:
        """批量索引"""
        results = {}
        for doc in documents:
            try:
                count = await self.index_document(doc)
                results[doc.id] = count
            except Exception as e:
                logger.error(f"Failed to index document {doc.id}: {e}")
                results[doc.id] = -1
        
        return results
```

---

## 四、检索引擎实现

### 4.1 混合检索

```python
# src/core/rag/retriever.py
from typing import List, Dict, Any, Tuple
from dataclasses import dataclass
import jieba
import math
from collections import Counter
from utils.logging import get_logger

logger = get_logger(__name__)


@dataclass
class SearchResult:
    """搜索结果"""
    chunk_id: str
    document_id: str
    chunk_text: str
    score: float
    metadata: Dict[str, Any]
    highlights: List[str] = None


class HybridSearchEngine:
    """混合检索引擎"""
    
    def __init__(
        self,
        vectorizer: EmbeddingEngine,
        vector_db: MilvusClient,
        semantic_weight: float = 0.7,
        keyword_weight: float = 0.3
    ):
        """初始化
        
        Args:
            vectorizer: 向量化引擎
            vector_db: 向量数据库
            semantic_weight: 语义检索权重
            keyword_weight: 关键词检索权重
        """
        self.vectorizer = vectorizer
        self.vector_db = vector_db
        self.semantic_weight = semantic_weight
        self.keyword_weight = keyword_weight
    
    async def search(
        self,
        query: str,
        user_id: str,
        project_id: str,
        top_k: int = 10,
        content_types: List[str] = None,
        enable_hybrid: bool = True
    ) -> List[SearchResult]:
        """混合检索
        
        Args:
            query: 查询文本
            user_id: 用户 ID
            project_id: 项目 ID
            top_k: 返回数量
            content_types: 内容类型过滤
            enable_hybrid: 是否启用混合检索
            
        Returns:
            搜索结果列表
        """
        logger.info(f"Hybrid search: query='{query[:50]}', top_k={top_k}")
        
        if not enable_hybrid:
            # 只使用语义检索
            return await self._semantic_search(query, user_id, project_id, top_k, content_types)
        
        # 1. 语义检索
        semantic_results = await self._semantic_search(
            query, user_id, project_id, top_k * 2, content_types
        )
        
        # 2. 关键词检索（BM25）
        keyword_results = await self._keyword_search(
            query, semantic_results, top_k * 2
        )
        
        # 3. 融合结果（RRF）
        fused_results = self._fuse_results(
            semantic_results,
            keyword_results,
            top_k
        )
        
        logger.info(f"Hybrid search returned {len(fused_results)} results")
        return fused_results
    
    async def _semantic_search(
        self,
        query: str,
        user_id: str,
        project_id: str,
        top_k: int,
        content_types: List[str] = None
    ) -> List[SearchResult]:
        """语义检索"""
        # 向量化查询
        query_vector = self.vectorizer.encode_query(query)
        
        # 构建过滤条件
        filters = {
            "user_id": user_id,
            "project_id": project_id
        }
        if content_types:
            filters["content_type"] = content_types
        
        # 检索
        hits = self.vector_db.search(
            query_vector=query_vector.tolist(),
            top_k=top_k,
            filters=filters
        )
        
        # 转换为 SearchResult
        results = []
        for chunk_id, score, metadata in hits:
            results.append(SearchResult(
                chunk_id=chunk_id,
                document_id=metadata["document_id"],
                chunk_text=metadata["chunk_text"],
                score=score,
                metadata=metadata
            ))
        
        return results
    
    async def _keyword_search(
        self,
        query: str,
        candidates: List[SearchResult],
        top_k: int
    ) -> List[SearchResult]:
        """关键词检索（BM25）"""
        # 分词
        query_tokens = list(jieba.cut_for_search(query))
        
        # 计算 BM25 分数
        scored_results = []
        for result in candidates:
            doc_tokens = list(jieba.cut_for_search(result.chunk_text))
            score = self._bm25_score(query_tokens, doc_tokens, len(candidates))
            
            # 复制结果并更新分数
            new_result = SearchResult(
                chunk_id=result.chunk_id,
                document_id=result.document_id,
                chunk_text=result.chunk_text,
                score=score,
                metadata=result.metadata,
                highlights=self._find_highlights(query_tokens, result.chunk_text)
            )
            scored_results.append(new_result)
        
        # 排序
        scored_results.sort(key=lambda x: x.score, reverse=True)
        return scored_results[:top_k]
    
    def _bm25_score(
        self,
        query_tokens: List[str],
        doc_tokens: List[str],
        corpus_size: int,
        k1: float = 1.5,
        b: float = 0.75
    ) -> float:
        """BM25 打分"""
        doc_len = len(doc_tokens)
        avg_doc_len = 100  # 假设平均文档长度
        
        doc_freq = Counter(doc_tokens)
        score = 0.0
        
        for token in query_tokens:
            if token in doc_freq:
                tf = doc_freq[token]
                idf = math.log((corpus_size - 1 + 0.5) / (1 + 0.5))
                
                score += idf * (tf * (k1 + 1)) / (
                    tf + k1 * (1 - b + b * (doc_len / avg_doc_len))
                )
        
        return score
    
    def _fuse_results(
        self,
        semantic_results: List[SearchResult],
        keyword_results: List[SearchResult],
        top_k: int
    ) -> List[SearchResult]:
        """融合结果（RRF - Reciprocal Rank Fusion）"""
        # RRF 公式: score = sum(1 / (k + rank))
        k = 60  # RRF 常数
        
        scores = {}
        
        # 语义检索分数
        for rank, result in enumerate(semantic_results, 1):
            chunk_id = result.chunk_id
            if chunk_id not in scores:
                scores[chunk_id] = {
                    "result": result,
                    "score": 0.0
                }
            scores[chunk_id]["score"] += self.semantic_weight / (k + rank)
        
        # 关键词检索分数
        for rank, result in enumerate(keyword_results, 1):
            chunk_id = result.chunk_id
            if chunk_id not in scores:
                scores[chunk_id] = {
                    "result": result,
                    "score": 0.0
                }
            scores[chunk_id]["score"] += self.keyword_weight / (k + rank)
            
            # 合并 highlights
            if result.highlights:
                scores[chunk_id]["result"].highlights = result.highlights
        
        # 排序
        sorted_items = sorted(
            scores.values(),
            key=lambda x: x["score"],
            reverse=True
        )
        
        # 返回 top_k
        fused_results = []
        for item in sorted_items[:top_k]:
            result = item["result"]
            result.score = item["score"]  # 更新为融合分数
            fused_results.append(result)
        
        return fused_results
    
    def _find_highlights(self, query_tokens: List[str], text: str, window: int = 50) -> List[str]:
        """查找高亮片段"""
        highlights = []
        text_lower = text.lower()
        
        for token in query_tokens:
            pos = text_lower.find(token.lower())
            if pos != -1:
                start = max(0, pos - window)
                end = min(len(text), pos + len(token) + window)
                highlight = text[start:end]
                if start > 0:
                    highlight = "..." + highlight
                if end < len(text):
                    highlight = highlight + "..."
                highlights.append(highlight)
        
        return highlights[:3]  # 最多返回 3 个高亮
```

### 4.2 重排序

```python
# src/core/rag/reranker.py
from typing import List
from sentence_transformers import CrossEncoder
from models.domain import SearchResult
from utils.logging import get_logger

logger = get_logger(__name__)


class Reranker:
    """重排序器"""
    
    def __init__(self, model_name: str = "BAAI/bge-reranker-large"):
        """初始化"""
        logger.info(f"Loading reranker model: {model_name}")
        self.model = CrossEncoder(model_name)
        logger.info("Reranker model loaded")
    
    def rerank(
        self,
        query: str,
        results: List[SearchResult],
        top_k: int = None
    ) -> List[SearchResult]:
        """重排序
        
        Args:
            query: 查询文本
            results: 搜索结果
            top_k: 返回数量
            
        Returns:
            重排序后的结果
        """
        if not results:
            return results
        
        logger.info(f"Reranking {len(results)} results...")
        
        # 准备输入
        pairs = [[query, result.chunk_text] for result in results]
        
        # 计算相关性分数
        scores = self.model.predict(pairs)
        
        # 更新分数并排序
        for result, score in zip(results, scores):
            result.score = float(score)
        
        results.sort(key=lambda x: x.score, reverse=True)
        
        if top_k:
            results = results[:top_k]
        
        logger.info(f"Reranking completed, top score: {results[0].score:.4f}")
        return results
```

---

## 五、事件驱动索引

### 5.1 事件监听

```python
# src/core/rag/event_handlers.py
from typing import Dict, Any
from core.rag.knowledge_base import KnowledgeBaseManager
from models.domain import Document
from utils.logging import get_logger

logger = get_logger(__name__)


class RAGIndexEventHandler:
    """RAG 索引事件处理器"""
    
    def __init__(self, kb_manager: KnowledgeBaseManager):
        """初始化"""
        self.kb_manager = kb_manager
    
    async def handle_document_created(self, event_data: Dict[str, Any]):
        """处理文档创建事件"""
        logger.info(f"Handling document created event: {event_data.get('document_id')}")
        
        # 构建 Document
        document = Document(
            id=event_data["document_id"],
            user_id=event_data["user_id"],
            project_id=event_data["project_id"],
            content=event_data["content"],
            content_type=event_data["content_type"],
            metadata=event_data.get("metadata", {})
        )
        
        # 索引文档
        try:
            await self.kb_manager.index_document(document)
            logger.info(f"Document indexed: {document.id}")
        except Exception as e:
            logger.error(f"Failed to index document {document.id}: {e}")
    
    async def handle_document_updated(self, event_data: Dict[str, Any]):
        """处理文档更新事件"""
        logger.info(f"Handling document updated event: {event_data.get('document_id')}")
        
        document_id = event_data["document_id"]
        
        # 删除旧索引
        try:
            await self.kb_manager.delete_document(document_id)
        except Exception as e:
            logger.error(f"Failed to delete old index for {document_id}: {e}")
        
        # 重新索引
        document = Document(
            id=document_id,
            user_id=event_data["user_id"],
            project_id=event_data["project_id"],
            content=event_data["content"],
            content_type=event_data["content_type"],
            metadata=event_data.get("metadata", {})
        )
        
        try:
            await self.kb_manager.index_document(document)
            logger.info(f"Document re-indexed: {document_id}")
        except Exception as e:
            logger.error(f"Failed to re-index document {document_id}: {e}")
    
    async def handle_document_deleted(self, event_data: Dict[str, Any]):
        """处理文档删除事件"""
        document_id = event_data["document_id"]
        logger.info(f"Handling document deleted event: {document_id}")
        
        try:
            await self.kb_manager.delete_document(document_id)
            logger.info(f"Document index deleted: {document_id}")
        except Exception as e:
            logger.error(f"Failed to delete document index {document_id}: {e}")
```

---

## 六、性能优化

### 6.1 批量处理

- **批量向量化**：batch_size=32，提升 10 倍效率
- **批量插入**：减少网络往返次数
- **异步处理**：使用 asyncio 并发处理

### 6.2 索引优化

- **HNSW 索引**：快速近似搜索，trade-off 精度和速度
- **参数调优**：M=16, efConstruction=128, ef=64

### 6.3 缓存策略

```python
# 查询缓存
from functools import lru_cache

@lru_cache(maxsize=1000)
def cached_search(query: str, user_id: str, project_id: str, top_k: int):
    # 缓存热门查询
    pass
```

---

## 七、监控和指标

### 7.1 关键指标

- **检索延迟**：P50, P95, P99
- **检索准确率**：MRR, NDCG
- **索引速度**：文档/秒
- **存储空间**：向量数量、磁盘占用

### 7.2 Prometheus 指标

```python
# metrics
rag_search_duration = Histogram('rag_search_duration_seconds')
rag_index_duration = Histogram('rag_index_duration_seconds')
rag_search_results_count = Histogram('rag_search_results_count')
```

---

## 八、测试策略

### 8.1 准确率测试

- 构建测试集：query-document pairs
- 计算 MRR、NDCG
- A/B 测试不同检索策略

### 8.2 性能测试

- 并发检索压测
- 大规模索引测试
- 长时间稳定性测试

---

## 九、部署和配置

### 9.1 Milvus 部署

```yaml
# docker-compose.yaml
services:
  milvus:
    image: milvusdb/milvus:latest
    ports:
      - "19530:19530"
    environment:
      - ETCD_USE_EMBED=true
    volumes:
      - milvus-data:/var/lib/milvus
```

### 9.2 配置示例

```yaml
rag:
  vectorization:
    model: "BAAI/bge-large-zh-v1.5"
    batch_size: 32
    device: "cuda"
  
  chunking:
    chunk_size: 500
    chunk_overlap: 50
  
  search:
    top_k: 10
    semantic_weight: 0.7
    keyword_weight: 0.3
    enable_rerank: true
  
  milvus:
    host: "localhost"
    port: 19530
```

---

## 十、总结

RAG 检索增强系统是 AI 能力的核心基础设施，通过向量检索和混合搜索技术，为 Agent 提供精准的上下文增强能力。

**关键特性**：
- ✅ 高精度混合检索（语义 + 关键词）
- ✅ 智能分块和向量化
- ✅ 事件驱动实时索引
- ✅ 重排序优化
- ✅ 高性能和可扩展性

**后续工作**：
- 与 Agent 工作流集成
- RAG Tool 封装
- 性能优化和测试

---

**文档版本**: v1.0
**创建时间**: 2025-10-27
**维护者**: AI架构组
