# FastAPI 微服务架构设计

> **文档版本**: v1.0  
> **创建时间**: 2025-10-27  
> **实施状态**: 设计阶段  
> **负责人**: AI架构组

---

## 📋 文档概述

本文档详细设计青羽平台 Python AI 微服务的架构，基于 FastAPI 框架构建高性能异步服务，集成 gRPC、LangGraph Agent、RAG 系统等核心模块。

**适用范围**：
- Python AI Service 整体架构
- FastAPI 服务实现
- gRPC 服务定义
- 模块分层和依赖管理

---

## 🎯 设计目标

### 核心目标

1. **高性能异步架构**：基于 FastAPI + asyncio，支持高并发请求
2. **双协议支持**：同时提供 gRPC（Go 通信）和 RESTful API（调试/监控）
3. **模块化设计**：清晰的分层架构，松耦合、高内聚
4. **可扩展性**：易于添加新 Agent、新工具、新模型
5. **可观测性**：完整的日志、监控、追踪支持

### 非目标

- ❌ 不实现业务数据的持久化（由 Go 后端负责）
- ❌ 不负责用户认证授权（由 Go 后端中间件处理）
- ❌ 不直接对外暴露服务（通过 Go 代理层访问）

---

## 一、系统架构

### 1.1 总体架构图

```
┌────────────────────────────────────────────────────────────────┐
│               Python AI Service (FastAPI)                      │
│                                                                  │
│  ┌──────────────────────────────────────────────────────────┐  │
│  │           API Layer (FastAPI + gRPC)                     │  │
│  │                                                            │  │
│  │  ┌────────────────┐        ┌────────────────────────┐   │  │
│  │  │  gRPC Server   │        │  RESTful API (FastAPI) │   │  │
│  │  │  - AIAgent     │        │  - /health             │   │  │
│  │  │  - RAG         │        │  - /api/v1/agents/*    │   │  │
│  │  │  - Tool        │        │  - /api/v1/rag/*       │   │  │
│  │  └────────┬───────┘        └────────┬───────────────┘   │  │
│  └───────────┼──────────────────────────┼───────────────────┘  │
│              │                          │                       │
│  ┌───────────▼──────────────────────────▼───────────────────┐  │
│  │              Service Layer                                │  │
│  │                                                            │  │
│  │  ┌──────────────────┐  ┌──────────────────────────────┐ │  │
│  │  │  AgentService    │  │  RAGService                  │ │  │
│  │  │  - execute()     │  │  - search()                  │ │  │
│  │  │  - stream()      │  │  - index()                   │ │  │
│  │  └──────────┬───────┘  └──────────┬───────────────────┘ │  │
│  │             │                      │                     │  │
│  │  ┌──────────▼──────────────────────▼───────────────────┐ │  │
│  │  │  ToolService                                         │ │  │
│  │  │  - call_tool()                                       │ │  │
│  │  │  - register_tool()                                   │ │  │
│  │  └──────────────────────────────────────────────────────┘ │  │
│  └───────────────────────────┬──────────────────────────────┘  │
│                              │                                 │
│  ┌───────────────────────────▼──────────────────────────────┐  │
│  │              Core Layer                                   │  │
│  │                                                            │  │
│  │  ┌────────────────┐  ┌────────────────┐  ┌────────────┐ │  │
│  │  │ LangGraph      │  │ RAG Engine     │  │ Tool       │ │  │
│  │  │ Workflows      │  │                │  │ Registry   │ │  │
│  │  │ - Agents       │  │ - Vectorizer   │  │            │ │  │
│  │  │ - States       │  │ - Retriever    │  │ - MCP      │ │  │
│  │  │ - Nodes        │  │ - Reranker     │  │ - LangChain│ │  │
│  │  └────────────────┘  └────────────────┘  └────────────┘ │  │
│  └────────────────────────────┬─────────────────────────────┘  │
│                               │                                │
│  ┌────────────────────────────▼────────────────────────────┐  │
│  │         Infrastructure Layer                             │  │
│  │                                                            │  │
│  │  ┌──────────┐  ┌──────────┐  ┌──────────┐  ┌─────────┐ │  │
│  │  │ LLM      │  │ Vector   │  │ Go API   │  │ Config  │ │  │
│  │  │ Clients  │  │ DB       │  │ Client   │  │ Manager │ │  │
│  │  │ - OpenAI │  │ - Milvus │  │ - gRPC   │  │ - Env   │ │  │
│  │  │ - Claude │  │ - Qdrant │  │ - HTTP   │  │ - YAML  │ │  │
│  │  └──────────┘  └──────────┘  └──────────┘  └─────────┘ │  │
│  └────────────────────────────────────────────────────────────┘  │
│                                                                  │
│  ┌────────────────────────────────────────────────────────────┐  │
│  │         Observability Layer                                │  │
│  │  - Logging (structlog)                                     │  │
│  │  - Metrics (Prometheus)                                    │  │
│  │  - Tracing (OpenTelemetry)                                 │  │
│  └────────────────────────────────────────────────────────────┘  │
└──────────────────────────────────────────────────────────────────┘
```

### 1.2 项目目录结构

```
python_ai_service/
├── src/
│   ├── __init__.py
│   │
│   ├── api/                          # API 层
│   │   ├── __init__.py
│   │   ├── grpc/                     # gRPC 服务
│   │   │   ├── __init__.py
│   │   │   ├── agent_service.py      # Agent gRPC 实现
│   │   │   ├── rag_service.py        # RAG gRPC 实现
│   │   │   └── tool_service.py       # Tool gRPC 实现
│   │   │
│   │   └── rest/                     # RESTful API
│   │       ├── __init__.py
│   │       ├── main.py               # FastAPI 应用入口
│   │       ├── agents.py             # Agent 端点
│   │       ├── rag.py                # RAG 端点
│   │       └── health.py             # 健康检查
│   │
│   ├── services/                     # Service 层
│   │   ├── __init__.py
│   │   ├── agent_service.py          # Agent 服务
│   │   ├── rag_service.py            # RAG 服务
│   │   └── tool_service.py           # 工具服务
│   │
│   ├── core/                         # 核心层
│   │   ├── __init__.py
│   │   │
│   │   ├── agents/                   # LangGraph Agents
│   │   │   ├── __init__.py
│   │   │   ├── base.py               # Agent 基类
│   │   │   ├── workflows/            # 工作流定义
│   │   │   │   ├── creative.py       # 创作工作流
│   │   │   │   ├── outline.py        # 大纲工作流
│   │   │   │   └── a2a_pipeline.py   # A2A 流水线
│   │   │   │
│   │   │   ├── nodes/                # 工作流节点
│   │   │   │   ├── understanding.py
│   │   │   │   ├── retrieval.py
│   │   │   │   ├── generation.py
│   │   │   │   └── review.py
│   │   │   │
│   │   │   └── states/               # 状态定义
│   │   │       ├── creative_state.py
│   │   │       └── outline_state.py
│   │   │
│   │   ├── rag/                      # RAG 引擎
│   │   │   ├── __init__.py
│   │   │   ├── vectorizer.py         # 向量化引擎
│   │   │   ├── retriever.py          # 检索器
│   │   │   ├── reranker.py           # 重排序
│   │   │   └── knowledge_base.py     # 知识库管理
│   │   │
│   │   └── tools/                    # 工具层
│   │       ├── __init__.py
│   │       ├── base.py               # Tool 基类
│   │       ├── registry.py           # 工具注册表
│   │       │
│   │       ├── langchain/            # LangChain Tools
│   │       │   ├── character_tool.py
│   │       │   ├── outline_tool.py
│   │       │   ├── timeline_tool.py
│   │       │   └── rag_tool.py
│   │       │
│   │       └── mcp/                  # MCP Tools
│   │           ├── server.py
│   │           └── client.py
│   │
│   ├── infrastructure/               # 基础设施层
│   │   ├── __init__.py
│   │   ├── llm/                      # LLM 客户端
│   │   │   ├── __init__.py
│   │   │   ├── base.py
│   │   │   ├── openai_client.py
│   │   │   └── anthropic_client.py
│   │   │
│   │   ├── vector_db/                # 向量数据库
│   │   │   ├── __init__.py
│   │   │   ├── base.py
│   │   │   └── milvus_client.py
│   │   │
│   │   ├── go_api/                   # Go API 客户端
│   │   │   ├── __init__.py
│   │   │   ├── grpc_client.py
│   │   │   └── http_client.py
│   │   │
│   │   └── config/                   # 配置管理
│   │       ├── __init__.py
│   │       ├── settings.py
│   │       └── env_loader.py
│   │
│   ├── models/                       # 数据模型
│   │   ├── __init__.py
│   │   ├── requests.py               # 请求模型
│   │   ├── responses.py              # 响应模型
│   │   └── domain.py                 # 领域模型
│   │
│   └── utils/                        # 工具函数
│       ├── __init__.py
│       ├── logging.py                # 日志配置
│       ├── metrics.py                # 指标收集
│       └── tracing.py                # 链路追踪
│
├── proto/                            # gRPC Proto 文件
│   ├── ai_agent.proto
│   ├── rag.proto
│   └── tool.proto
│
├── tests/                            # 测试
│   ├── unit/
│   ├── integration/
│   └── e2e/
│
├── config/                           # 配置文件
│   ├── config.yaml
│   ├── config.dev.yaml
│   └── config.prod.yaml
│
├── docker/                           # Docker 相关
│   ├── Dockerfile
│   └── docker-compose.yaml
│
├── scripts/                          # 脚本
│   ├── generate_proto.sh
│   └── start_dev.sh
│
├── requirements.txt                  # Python 依赖
├── pyproject.toml                    # 项目配置
├── .env.example                      # 环境变量示例
└── README.md                         # 项目说明
```

---

## 二、分层架构设计

### 2.1 API 层（API Layer）

#### 2.1.1 gRPC Server

**职责**：
- 接收来自 Go 后端的 gRPC 请求
- 参数验证和转换
- 调用 Service 层处理业务逻辑
- 流式响应支持

**实现示例**：

```python
# src/api/grpc/agent_service.py
from grpc import aio
from proto import ai_agent_pb2, ai_agent_pb2_grpc
from services.agent_service import AgentService
from utils.logging import get_logger

logger = get_logger(__name__)


class AIAgentServicer(ai_agent_pb2_grpc.AIAgentServiceServicer):
    """AI Agent gRPC 服务实现"""
    
    def __init__(self, agent_service: AgentService):
        self.agent_service = agent_service
    
    async def ExecuteAgent(
        self,
        request: ai_agent_pb2.AgentRequest,
        context: aio.ServicerContext
    ) -> ai_agent_pb2.AgentResponse:
        """执行 Agent（同步）"""
        try:
            logger.info(
                "Agent execution started",
                agent_type=request.agent_type,
                task=request.task[:100]
            )
            
            # 调用 Service 层
            result = await self.agent_service.execute(
                agent_type=request.agent_type,
                task=request.task,
                context=dict(request.context),
                tools=list(request.tools),
                user_id=request.user_id,
                project_id=request.project_id
            )
            
            logger.info("Agent execution completed", result_length=len(result.output))
            
            # 构建响应
            return ai_agent_pb2.AgentResponse(
                output=result.output,
                tool_calls=[
                    ai_agent_pb2.ToolCall(
                        tool_name=tc.tool_name,
                        parameters=tc.parameters,
                        result=tc.result
                    )
                    for tc in result.tool_calls
                ],
                status=result.status,
                reasoning=result.reasoning
            )
            
        except Exception as e:
            logger.error("Agent execution failed", error=str(e), exc_info=True)
            await context.abort(
                grpc.StatusCode.INTERNAL,
                f"Agent execution failed: {str(e)}"
            )
    
    async def ExecuteAgentStream(
        self,
        request: ai_agent_pb2.AgentRequest,
        context: aio.ServicerContext
    ):
        """执行 Agent（流式）"""
        try:
            logger.info("Agent stream execution started", agent_type=request.agent_type)
            
            # 流式执行
            async for chunk in self.agent_service.execute_stream(
                agent_type=request.agent_type,
                task=request.task,
                context=dict(request.context),
                tools=list(request.tools)
            ):
                yield ai_agent_pb2.AgentStreamChunk(
                    delta=chunk.delta,
                    chunk_type=chunk.chunk_type,
                    metadata=chunk.metadata
                )
                
        except Exception as e:
            logger.error("Agent stream execution failed", error=str(e))
            raise
```

#### 2.1.2 RESTful API

**职责**：
- 提供 HTTP 接口用于调试和监控
- 健康检查端点
- Swagger/OpenAPI 文档

**实现示例**：

```python
# src/api/rest/main.py
from fastapi import FastAPI, Depends
from fastapi.middleware.cors import CORSMiddleware
from contextlib import asynccontextmanager

from infrastructure.config import get_settings
from services import AgentService, RAGService, ToolService
from api.rest import agents, rag, health
from utils.logging import setup_logging
from utils.metrics import setup_metrics

# 生命周期管理
@asynccontextmanager
async def lifespan(app: FastAPI):
    """应用生命周期管理"""
    # 启动时初始化
    setup_logging()
    setup_metrics()
    
    logger.info("Python AI Service starting up...")
    
    # 初始化服务
    await app.state.agent_service.initialize()
    await app.state.rag_service.initialize()
    
    logger.info("Python AI Service started successfully")
    
    yield
    
    # 关闭时清理
    logger.info("Python AI Service shutting down...")
    await app.state.agent_service.close()
    await app.state.rag_service.close()
    logger.info("Python AI Service stopped")


# 创建 FastAPI 应用
def create_app() -> FastAPI:
    """创建 FastAPI 应用"""
    settings = get_settings()
    
    app = FastAPI(
        title="Qingyu AI Service",
        version="1.0.0",
        description="青羽平台 AI 服务",
        lifespan=lifespan
    )
    
    # CORS 中间件
    app.add_middleware(
        CORSMiddleware,
        allow_origins=settings.cors_origins,
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )
    
    # 初始化服务（依赖注入）
    app.state.agent_service = AgentService()
    app.state.rag_service = RAGService()
    app.state.tool_service = ToolService()
    
    # 注册路由
    app.include_router(health.router, tags=["Health"])
    app.include_router(agents.router, prefix="/api/v1/agents", tags=["Agents"])
    app.include_router(rag.router, prefix="/api/v1/rag", tags=["RAG"])
    
    return app


app = create_app()


# src/api/rest/agents.py
from fastapi import APIRouter, HTTPException, Request
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from typing import List, Dict, Any, Optional

router = APIRouter()


class AgentExecuteRequest(BaseModel):
    """Agent 执行请求"""
    agent_type: str
    task: str
    context: Optional[Dict[str, Any]] = {}
    tools: Optional[List[str]] = []
    stream: bool = False


class AgentExecuteResponse(BaseModel):
    """Agent 执行响应"""
    output: str
    tool_calls: List[Dict[str, Any]]
    status: str
    reasoning: List[str]


@router.post("/execute", response_model=AgentExecuteResponse)
async def execute_agent(
    request: AgentExecuteRequest,
    app_request: Request
):
    """执行 Agent"""
    agent_service = app_request.app.state.agent_service
    
    try:
        if request.stream:
            # 流式响应
            async def generate():
                async for chunk in agent_service.execute_stream(
                    agent_type=request.agent_type,
                    task=request.task,
                    context=request.context,
                    tools=request.tools
                ):
                    yield f"data: {chunk.model_dump_json()}\n\n"
            
            return StreamingResponse(
                generate(),
                media_type="text/event-stream"
            )
        else:
            # 同步响应
            result = await agent_service.execute(
                agent_type=request.agent_type,
                task=request.task,
                context=request.context,
                tools=request.tools
            )
            
            return AgentExecuteResponse(
                output=result.output,
                tool_calls=[tc.dict() for tc in result.tool_calls],
                status=result.status,
                reasoning=result.reasoning
            )
            
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/health")
async def agent_health(app_request: Request):
    """Agent 服务健康检查"""
    agent_service = app_request.app.state.agent_service
    health_status = await agent_service.health_check()
    
    return {
        "status": "healthy" if health_status.healthy else "unhealthy",
        "details": health_status.details
    }
```

### 2.2 Service 层（Service Layer）

**职责**：
- 业务逻辑编排
- 调用 Core 层的 Agent、RAG、Tool
- 错误处理和重试
- 结果聚合和转换

**实现示例**：

```python
# src/services/agent_service.py
from typing import Dict, Any, List, AsyncGenerator
from dataclasses import dataclass

from core.agents.workflows import creative, outline, a2a_pipeline
from core.tools.registry import ToolRegistry
from infrastructure.llm import get_llm_client
from utils.logging import get_logger

logger = get_logger(__name__)


@dataclass
class AgentExecutionResult:
    """Agent 执行结果"""
    output: str
    tool_calls: List[Dict[str, Any]]
    status: str
    reasoning: List[str]


class AgentService:
    """Agent 服务"""
    
    def __init__(self):
        self.tool_registry = ToolRegistry()
        self.workflows = {
            "creative": creative.create_workflow(),
            "outline": outline.create_workflow(),
            "a2a_pipeline": a2a_pipeline.create_workflow()
        }
        self.llm_client = None
    
    async def initialize(self):
        """初始化服务"""
        logger.info("Initializing AgentService...")
        
        # 初始化 LLM 客户端
        self.llm_client = get_llm_client()
        
        # 注册工具
        await self.tool_registry.register_all()
        
        logger.info("AgentService initialized")
    
    async def execute(
        self,
        agent_type: str,
        task: str,
        context: Dict[str, Any],
        tools: List[str],
        user_id: str = None,
        project_id: str = None
    ) -> AgentExecutionResult:
        """执行 Agent（同步）"""
        try:
            logger.info(
                "Executing agent",
                agent_type=agent_type,
                task_length=len(task),
                tools=tools
            )
            
            # 获取工作流
            workflow = self.workflows.get(agent_type)
            if not workflow:
                raise ValueError(f"Unknown agent type: {agent_type}")
            
            # 准备初始状态
            initial_state = {
                "task": task,
                "context": context,
                "tools": tools,
                "user_id": user_id,
                "project_id": project_id,
                "messages": [],
                "tool_calls": [],
                "reasoning": []
            }
            
            # 执行工作流
            result = await workflow.ainvoke(
                initial_state,
                config={"configurable": {"thread_id": f"{user_id}_{project_id}"}}
            )
            
            logger.info("Agent execution completed", status=result.get("status"))
            
            return AgentExecutionResult(
                output=result.get("output", ""),
                tool_calls=result.get("tool_calls", []),
                status=result.get("status", "completed"),
                reasoning=result.get("reasoning", [])
            )
            
        except Exception as e:
            logger.error("Agent execution failed", error=str(e), exc_info=True)
            raise
    
    async def execute_stream(
        self,
        agent_type: str,
        task: str,
        context: Dict[str, Any],
        tools: List[str]
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """执行 Agent（流式）"""
        try:
            workflow = self.workflows.get(agent_type)
            if not workflow:
                raise ValueError(f"Unknown agent type: {agent_type}")
            
            initial_state = {
                "task": task,
                "context": context,
                "tools": tools,
                "messages": [],
                "tool_calls": [],
                "reasoning": []
            }
            
            # 流式执行
            async for event in workflow.astream_events(initial_state, version="v1"):
                if event["event"] == "on_chat_model_stream":
                    chunk = event["data"]["chunk"]
                    yield {
                        "delta": chunk.content,
                        "chunk_type": "text",
                        "metadata": {}
                    }
                
                elif event["event"] == "on_tool_start":
                    yield {
                        "delta": "",
                        "chunk_type": "tool_start",
                        "metadata": {
                            "tool_name": event["name"]
                        }
                    }
                
                elif event["event"] == "on_tool_end":
                    yield {
                        "delta": "",
                        "chunk_type": "tool_end",
                        "metadata": {
                            "tool_name": event["name"],
                            "result": event["data"]
                        }
                    }
                    
        except Exception as e:
            logger.error("Agent stream execution failed", error=str(e))
            raise
    
    async def health_check(self):
        """健康检查"""
        return {
            "healthy": True,
            "details": {
                "workflows": list(self.workflows.keys()),
                "tools": self.tool_registry.list_tools()
            }
        }
    
    async def close(self):
        """关闭服务"""
        logger.info("Closing AgentService...")
        if self.llm_client:
            await self.llm_client.close()
```

### 2.3 Core 层（Core Layer）

**职责**：
- LangGraph Agent 工作流定义
- RAG 引擎实现
- Tool 注册和管理

**详细设计见后续文档**：
- 文档4：LangGraph Agent 工作流架构
- 文档5：A2A 创作流水线设计
- 文档7：LangChain Tools 实现
- 文档8：RAG 检索增强系统实现

### 2.4 Infrastructure 层（Infrastructure Layer）

**职责**：
- LLM 客户端封装
- 向量数据库客户端
- Go API 客户端
- 配置管理

**实现示例**：

```python
# src/infrastructure/llm/base.py
from abc import ABC, abstractmethod
from typing import List, Dict, Any, AsyncGenerator


class BaseLLMClient(ABC):
    """LLM 客户端基类"""
    
    @abstractmethod
    async def generate(
        self,
        messages: List[Dict[str, str]],
        model: str = None,
        temperature: float = 0.7,
        max_tokens: int = 2000,
        **kwargs
    ) -> str:
        """生成文本"""
        pass
    
    @abstractmethod
    async def generate_stream(
        self,
        messages: List[Dict[str, str]],
        model: str = None,
        temperature: float = 0.7,
        **kwargs
    ) -> AsyncGenerator[str, None]:
        """流式生成文本"""
        pass
    
    @abstractmethod
    async def embed(self, texts: List[str]) -> List[List[float]]:
        """文本向量化"""
        pass


# src/infrastructure/llm/openai_client.py
from openai import AsyncOpenAI
from typing import List, Dict, Any, AsyncGenerator

from .base import BaseLLMClient
from utils.logging import get_logger

logger = get_logger(__name__)


class OpenAIClient(BaseLLMClient):
    """OpenAI 客户端"""
    
    def __init__(self, api_key: str, base_url: str = None):
        self.client = AsyncOpenAI(api_key=api_key, base_url=base_url)
        self.default_model = "gpt-4-turbo-preview"
    
    async def generate(
        self,
        messages: List[Dict[str, str]],
        model: str = None,
        temperature: float = 0.7,
        max_tokens: int = 2000,
        **kwargs
    ) -> str:
        """生成文本"""
        try:
            response = await self.client.chat.completions.create(
                model=model or self.default_model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                **kwargs
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            logger.error("OpenAI generation failed", error=str(e))
            raise
    
    async def generate_stream(
        self,
        messages: List[Dict[str, str]],
        model: str = None,
        temperature: float = 0.7,
        **kwargs
    ) -> AsyncGenerator[str, None]:
        """流式生成文本"""
        try:
            stream = await self.client.chat.completions.create(
                model=model or self.default_model,
                messages=messages,
                temperature=temperature,
                stream=True,
                **kwargs
            )
            
            async for chunk in stream:
                if chunk.choices[0].delta.content:
                    yield chunk.choices[0].delta.content
                    
        except Exception as e:
            logger.error("OpenAI stream generation failed", error=str(e))
            raise
    
    async def embed(self, texts: List[str]) -> List[List[float]]:
        """文本向量化"""
        try:
            response = await self.client.embeddings.create(
                model="text-embedding-3-large",
                input=texts
            )
            
            return [data.embedding for data in response.data]
            
        except Exception as e:
            logger.error("OpenAI embedding failed", error=str(e))
            raise
```

---

## 三、配置管理

### 3.1 配置结构

```python
# src/infrastructure/config/settings.py
from pydantic import BaseSettings, Field
from typing import List, Optional


class Settings(BaseSettings):
    """应用配置"""
    
    # 基础配置
    app_name: str = "Qingyu AI Service"
    app_version: str = "1.0.0"
    environment: str = Field(default="development", env="ENV")
    debug: bool = Field(default=False, env="DEBUG")
    
    # 服务配置
    grpc_host: str = Field(default="0.0.0.0", env="GRPC_HOST")
    grpc_port: int = Field(default=50051, env="GRPC_PORT")
    rest_host: str = Field(default="0.0.0.0", env="REST_HOST")
    rest_port: int = Field(default=8000, env="REST_PORT")
    
    # Go 后端配置
    go_backend_url: str = Field(..., env="GO_BACKEND_URL")
    go_backend_grpc_url: str = Field(..., env="GO_BACKEND_GRPC_URL")
    
    # LLM 配置
    openai_api_key: str = Field(..., env="OPENAI_API_KEY")
    openai_base_url: Optional[str] = Field(None, env="OPENAI_BASE_URL")
    anthropic_api_key: Optional[str] = Field(None, env="ANTHROPIC_API_KEY")
    default_llm_provider: str = Field(default="openai", env="DEFAULT_LLM_PROVIDER")
    default_llm_model: str = Field(default="gpt-4-turbo-preview", env="DEFAULT_LLM_MODEL")
    
    # 向量数据库配置
    milvus_host: str = Field(..., env="MILVUS_HOST")
    milvus_port: int = Field(default=19530, env="MILVUS_PORT")
    milvus_user: Optional[str] = Field(None, env="MILVUS_USER")
    milvus_password: Optional[str] = Field(None, env="MILVUS_PASSWORD")
    
    # 向量模型配置
    embedding_model: str = Field(default="BAAI/bge-large-zh-v1.5", env="EMBEDDING_MODEL")
    embedding_batch_size: int = Field(default=32, env="EMBEDDING_BATCH_SIZE")
    
    # 日志配置
    log_level: str = Field(default="INFO", env="LOG_LEVEL")
    log_format: str = Field(default="json", env="LOG_FORMAT")
    
    # 监控配置
    enable_metrics: bool = Field(default=True, env="ENABLE_METRICS")
    metrics_port: int = Field(default=9090, env="METRICS_PORT")
    
    # CORS 配置
    cors_origins: List[str] = Field(
        default=["http://localhost:3000"],
        env="CORS_ORIGINS"
    )
    
    class Config:
        env_file = ".env"
        case_sensitive = False


_settings: Optional[Settings] = None


def get_settings() -> Settings:
    """获取配置（单例）"""
    global _settings
    if _settings is None:
        _settings = Settings()
    return _settings
```

### 3.2 配置文件

```yaml
# config/config.yaml
app:
  name: "Qingyu AI Service"
  version: "1.0.0"
  environment: "development"

grpc:
  host: "0.0.0.0"
  port: 50051

rest:
  host: "0.0.0.0"
  port: 8000

go_backend:
  url: "http://localhost:8080"
  grpc_url: "localhost:50052"

llm:
  default_provider: "openai"
  default_model: "gpt-4-turbo-preview"
  openai:
    base_url: null
  anthropic:
    base_url: null

vector_db:
  provider: "milvus"
  milvus:
    host: "localhost"
    port: 19530

embedding:
  model: "BAAI/bge-large-zh-v1.5"
  batch_size: 32

logging:
  level: "INFO"
  format: "json"

metrics:
  enabled: true
  port: 9090
```

---

## 四、启动流程

### 4.1 服务初始化

```python
# src/main.py
import asyncio
from concurrent import futures
import grpc
from grpc import aio

from api.rest.main import create_app as create_rest_app
from api.grpc import agent_service, rag_service, tool_service
from proto import ai_agent_pb2_grpc, rag_pb2_grpc, tool_pb2_grpc
from services import AgentService, RAGService, ToolService
from infrastructure.config import get_settings
from utils.logging import setup_logging, get_logger

logger = get_logger(__name__)


async def serve():
    """启动服务"""
    settings = get_settings()
    
    # 设置日志
    setup_logging(level=settings.log_level)
    
    logger.info("Starting Qingyu AI Service...", environment=settings.environment)
    
    # 初始化服务
    agent_svc = AgentService()
    rag_svc = RAGService()
    tool_svc = ToolService()
    
    await agent_svc.initialize()
    await rag_svc.initialize()
    await tool_svc.initialize()
    
    logger.info("Services initialized")
    
    # 创建 gRPC Server
    grpc_server = aio.server(futures.ThreadPoolExecutor(max_workers=10))
    
    # 注册 gRPC 服务
    ai_agent_pb2_grpc.add_AIAgentServiceServicer_to_server(
        agent_service.AIAgentServicer(agent_svc),
        grpc_server
    )
    rag_pb2_grpc.add_RAGServiceServicer_to_server(
        rag_service.RAGServicer(rag_svc),
        grpc_server
    )
    tool_pb2_grpc.add_ToolServiceServicer_to_server(
        tool_service.ToolServicer(tool_svc),
        grpc_server
    )
    
    # 启动 gRPC Server
    grpc_server.add_insecure_port(f"{settings.grpc_host}:{settings.grpc_port}")
    await grpc_server.start()
    
    logger.info(f"gRPC server started on {settings.grpc_host}:{settings.grpc_port}")
    
    # 创建 REST API (FastAPI)
    rest_app = create_rest_app()
    rest_app.state.agent_service = agent_svc
    rest_app.state.rag_service = rag_svc
    rest_app.state.tool_service = tool_svc
    
    # 启动 REST API
    import uvicorn
    config = uvicorn.Config(
        rest_app,
        host=settings.rest_host,
        port=settings.rest_port,
        log_level=settings.log_level.lower()
    )
    server = uvicorn.Server(config)
    
    logger.info(f"REST API server starting on {settings.rest_host}:{settings.rest_port}")
    
    # 同时运行 gRPC 和 REST
    await asyncio.gather(
        server.serve(),
        grpc_server.wait_for_termination()
    )


if __name__ == "__main__":
    asyncio.run(serve())
```

### 4.2 启动脚本

```bash
# scripts/start_dev.sh
#!/bin/bash

# 设置环境变量
export ENV=development
export DEBUG=true

# 启动服务
python -m src.main
```

---

## 五、依赖管理

### 5.1 requirements.txt

```txt
# Web 框架
fastapi==0.104.1
uvicorn[standard]==0.24.0
grpcio==1.59.0
grpcio-tools==1.59.0

# LangChain 生态
langchain==0.1.0
langchain-openai==0.0.2
langchain-anthropic==0.0.1
langgraph==0.0.20

# LLM 客户端
openai==1.6.1
anthropic==0.8.1

# 向量和 Embedding
sentence-transformers==2.2.2
pymilvus==2.3.4

# HTTP 客户端
httpx==0.25.2
aiohttp==3.9.1

# 数据验证
pydantic==2.5.2
pydantic-settings==2.1.0

# 日志和监控
structlog==23.2.0
prometheus-client==0.19.0
opentelemetry-api==1.21.0
opentelemetry-sdk==1.21.0
opentelemetry-instrumentation-fastapi==0.42b0

# 工具
python-dotenv==1.0.0
PyYAML==6.0.1
tenacity==8.2.3

# 开发依赖
pytest==7.4.3
pytest-asyncio==0.21.1
pytest-cov==4.1.0
black==23.12.1
flake8==6.1.0
mypy==1.7.1
```

---

## 六、监控和可观测性

### 6.1 日志配置

```python
# src/utils/logging.py
import structlog
import logging
import sys


def setup_logging(level: str = "INFO"):
    """配置结构化日志"""
    
    # 配置 structlog
    structlog.configure(
        processors=[
            structlog.stdlib.filter_by_level,
            structlog.stdlib.add_logger_name,
            structlog.stdlib.add_log_level,
            structlog.stdlib.PositionalArgumentsFormatter(),
            structlog.processors.TimeStamper(fmt="iso"),
            structlog.processors.StackInfoRenderer(),
            structlog.processors.format_exc_info,
            structlog.processors.UnicodeDecoder(),
            structlog.processors.JSONRenderer()
        ],
        wrapper_class=structlog.stdlib.BoundLogger,
        context_class=dict,
        logger_factory=structlog.stdlib.LoggerFactory(),
        cache_logger_on_first_use=True,
    )
    
    # 配置标准库 logging
    logging.basicConfig(
        format="%(message)s",
        stream=sys.stdout,
        level=getattr(logging, level.upper())
    )


def get_logger(name: str):
    """获取日志器"""
    return structlog.get_logger(name)
```

### 6.2 Prometheus 指标

```python
# src/utils/metrics.py
from prometheus_client import Counter, Histogram, Gauge, start_http_server


# 定义指标
agent_requests_total = Counter(
    'agent_requests_total',
    'Total number of agent requests',
    ['agent_type', 'status']
)

agent_duration_seconds = Histogram(
    'agent_duration_seconds',
    'Agent execution duration in seconds',
    ['agent_type']
)

rag_search_duration_seconds = Histogram(
    'rag_search_duration_seconds',
    'RAG search duration in seconds'
)

tool_calls_total = Counter(
    'tool_calls_total',
    'Total number of tool calls',
    ['tool_name', 'status']
)

active_workflows = Gauge(
    'active_workflows',
    'Number of active workflows'
)


def setup_metrics(port: int = 9090):
    """启动 Prometheus 指标服务器"""
    start_http_server(port)
```

---

## 七、部署配置

### 7.1 Dockerfile

```dockerfile
# docker/Dockerfile
FROM python:3.11-slim

WORKDIR /app

# 安装系统依赖
RUN apt-get update && apt-get install -y \
    build-essential \
    git \
    && rm -rf /var/lib/apt/lists/*

# 复制依赖文件
COPY requirements.txt .

# 安装 Python 依赖
RUN pip install --no-cache-dir -r requirements.txt

# 复制代码
COPY src/ ./src/
COPY proto/ ./proto/
COPY config/ ./config/

# 生成 gRPC 代码
RUN python -m grpc_tools.protoc \
    -I./proto \
    --python_out=./src \
    --grpc_python_out=./src \
    proto/*.proto

# 暴露端口
EXPOSE 50051 8000 9090

# 启动命令
CMD ["python", "-m", "src.main"]
```

### 7.2 Docker Compose

```yaml
# docker/docker-compose.yaml
version: '3.8'

services:
  ai-service:
    build:
      context: ..
      dockerfile: docker/Dockerfile
    container_name: qingyu-ai-service
    ports:
      - "50051:50051"  # gRPC
      - "8000:8000"    # REST API
      - "9090:9090"    # Prometheus metrics
    environment:
      - ENV=production
      - DEBUG=false
      - GO_BACKEND_URL=http://go-backend:8080
      - GO_BACKEND_GRPC_URL=go-backend:50052
      - MILVUS_HOST=milvus
      - MILVUS_PORT=19530
      - OPENAI_API_KEY=${OPENAI_API_KEY}
    volumes:
      - ./config:/app/config
    depends_on:
      - milvus
    restart: unless-stopped
  
  milvus:
    image: milvusdb/milvus:latest
    container_name: milvus-standalone
    ports:
      - "19530:19530"
    environment:
      - ETCD_USE_EMBED=true
      - COMMON_STORAGETYPE=local
    volumes:
      - milvus-data:/var/lib/milvus
    restart: unless-stopped

volumes:
  milvus-data:
```

---

## 八、测试策略

### 8.1 单元测试

```python
# tests/unit/test_agent_service.py
import pytest
from unittest.mock import AsyncMock, MagicMock

from services.agent_service import AgentService


@pytest.mark.asyncio
async def test_agent_execute():
    """测试 Agent 执行"""
    service = AgentService()
    service.workflows = {
        "creative": AsyncMock(return_value={
            "output": "Test output",
            "status": "completed",
            "tool_calls": [],
            "reasoning": []
        })
    }
    
    result = await service.execute(
        agent_type="creative",
        task="Test task",
        context={},
        tools=[]
    )
    
    assert result.output == "Test output"
    assert result.status == "completed"
```

### 8.2 集成测试

```python
# tests/integration/test_agent_workflow.py
import pytest
from services.agent_service import AgentService


@pytest.mark.asyncio
async def test_creative_workflow_integration():
    """测试创作工作流集成"""
    service = AgentService()
    await service.initialize()
    
    result = await service.execute(
        agent_type="creative",
        task="创建一个武侠小说大纲",
        context={"genre": "武侠", "length": "中篇"},
        tools=["outline_tool", "character_tool"]
    )
    
    assert result.status == "completed"
    assert len(result.output) > 0
    assert len(result.tool_calls) > 0
```

---

## 九、最佳实践

### 9.1 错误处理

- 使用自定义异常类
- 统一的错误响应格式
- 详细的错误日志
- 优雅降级

### 9.2 性能优化

- 异步 I/O
- 连接池管理
- 批量处理
- 缓存策略

### 9.3 安全考虑

- API 密钥管理（环境变量）
- 请求验证
- 速率限制
- 敏感信息脱敏

---

## 十、后续工作

- [ ] 完善 gRPC Proto 定义
- [ ] 实现完整的 LangGraph 工作流
- [ ] 集成 RAG 系统
- [ ] 实现工具注册和管理
- [ ] 性能测试和优化
- [ ] 完整的文档和示例

---

**文档版本**: v1.0
**创建时间**: 2025-10-27
**维护者**: AI架构组
